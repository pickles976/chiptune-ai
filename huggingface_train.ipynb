{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZMbGCw0Qxfg"
      },
      "source": [
        "# **Finetuning GPT2 using HuggingFace and Tensorflow**\n",
        "\n",
        "In this colab notebook we set up a simple outline of how you can use Huggingface to fine tune a gpt2 model on finance titles to generate new possible headlines. This notebook uses the hugginface finefuning scripts and then uses the TensorFlow version of the genreated models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzB7m0GI5fSt"
      },
      "source": [
        "First begin setup by cloning transformers repo. We need to store the training script locally since there isnt an easier way to train tf based gpt2 models as far as I can see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91rmSAUQVIUP",
        "outputId": "a5564ff3-d730-4244-a168-f61916d777da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#Clone the transformers repo into the notebook\n",
        "!git clone https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFzEr4Y1VJKP",
        "outputId": "697d516b-d45d-49e8-bea9-b56695be4d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  transformers\n"
          ]
        }
      ],
      "source": [
        "# Clone should now be in the machine\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-ke4920tDqv"
      },
      "source": [
        "Check to see what gpu we were granted. For Colab Pro it will vary between a Tesla V100 or P100. For normal colab it should be a k80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPIgByLqmI81",
        "outputId": "aeb06ae3-5387-47d0-c4f3-3ad9197c539c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 18 21:47:04 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOTdb4rWv8YN"
      },
      "source": [
        "Change directory location to be in the examples folder and then install any requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2M2Oz9CYB4P",
        "outputId": "bcd6c9dc-a7ce-4175-966d-6012db2651b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md\t  run_clm_no_trainer.py  run_mlm_no_trainer.py\trun_plm.py\n",
            "requirements.txt  run_clm.py\t\t run_mlm.py\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(\"/content/transformers\")\n",
        "os.chdir(\"./examples/pytorch/language-modeling\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04rBGxwiYnep",
        "outputId": "f6827d3e-f667-4897-9c24-3f0a3f51f2e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.6.2-py3-none-any.whl (65 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████                           | 10 kB 36.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20 kB 37.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 30 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 51 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 61 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 65 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.11.0+cu113)\n",
            "Collecting datasets>=1.8.0\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 30.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r requirements.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (4.11.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 57.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (4.64.0)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 77.2 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (0.70.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 3)) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 3)) (2021.10.8)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 89.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r requirements.txt (line 5)) (1.15.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 93.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 90.5 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.8.0->-r requirements.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, sentencepiece, datasets, accelerate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed accelerate-0.6.2 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 responses-0.18.0 sentencepiece-0.1.96 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB29mAKjaNIQ",
        "outputId": "2971b6e7-4132-4db4-b8ce-dd46cb8aa1c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md\t  run_clm_no_trainer.py  run_mlm_no_trainer.py\trun_plm.py\n",
            "requirements.txt  run_clm.py\t\t run_mlm.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo5gRmXaWx0m",
        "outputId": "92690b74-dfac-4587-c515-ed4854a7b6c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.7 MB 86.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow) (1.21.6)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 6.0.1\n",
            "    Uninstalling pyarrow-6.0.1:\n",
            "      Successfully uninstalled pyarrow-6.0.1\n",
            "Successfully installed pyarrow-7.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5sdYSpAWY1S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/transformers/examples/pytorch/\")\n",
        "os.chdir(\"./language-modeling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6G6WINaYmwx",
        "outputId": "667d784e-da09-4a41-990f-dd6bc422e84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-moenmb58\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-moenmb58\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.5.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 14.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 72.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.25.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.1.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=4041285 sha256=bea1e21e4c79afb6c6dd031bb72caa319e4843b175d1e7d026557c11eed5c42f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f23v7_h3/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.19.0.dev0\n"
          ]
        }
      ],
      "source": [
        "# Need to install latest transformer packages from github so the scripts will run correctly\n",
        "! pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSkcq_Xv8bYD"
      },
      "source": [
        "Mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfZZh2h-8cV9",
        "outputId": "e760464d-a6ec-4dd2-d3d0-a682229e884d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TYi8Oqs6Npo"
      },
      "source": [
        "The script below will fine tune GPT2 on your text data that you setup above. This training step will take anywhre from tens of minutes to hours depending on how large your training set is, how many epochs you intend to train on, and if you are using colab or colab pro. We utilize mixed precision in this model to shave off some training time. For a large data set I was using for another experiment it saved us over 30 mins in training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM7MN8VMAMfP"
      },
      "source": [
        "## Initializing a tokenizer:\n",
        "#### (only needs to be done once)\n",
        "Run the tokenizer on the lakh_dataset. \n",
        "Even though we will not be using the full dataset,\n",
        "we will need the tokenizer to ensure that our dataset\n",
        "entries have the proper length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGcehUvLaQ0F"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=\"/content/lakh_dataset.txt\", vocab_size=8192, min_frequency=2,\n",
        "                show_progress=True,\n",
        "                special_tokens=[\"<|endoftext|>\"])\n",
        "#Save the Tokenizer to disk\n",
        "tokenizer.save_model(\"/content/gdrive/MyDrive/gpt2/\")\n",
        "tokenizer.save(\"/content/gdrive/MyDrive/gpt2/tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Datasets with the proper token length\n",
        "#### 1. After generating the Tokenizer, loop through the ABC dataset\n",
        "#### 2. Keep all .abc files with less than 8192 spaces\n",
        "#### 3. Check all remaining .abc files token length\n",
        "#### 4. Add songs with less than 2048 tokens to the dataset"
      ],
      "metadata": {
        "id": "sq6PSlqGI4v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test that our encoder is working\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/gdrive/MyDrive/gpt2\")\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "prompt = \"\"\"X:1\n",
        "T:Music21 Fragment\n",
        "C:Music21\n",
        "%%score\"\"\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "\n",
        "print(input_ids[0])\n",
        "print(len(input_ids[0]))"
      ],
      "metadata": {
        "id": "9Fl-GXfbMxtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd85da9-f8b5-4a6b-9fd9-315c0b11fbbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[ 56  26  17 199  52  26  45  85 293 284 221  38 295 332 199  35  26  45\n",
            "  85 293 284 199 346 350], shape=(24,), dtype=int32)\n",
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import random\n",
        "from transformers import GPT2Tokenizer\n",
        "import os\n",
        "\n",
        "indir = \"/content/gdrive/MyDrive/ALL_ABC\"\n",
        "# indir = \"/content/gdrive/MyDrive/NES_DB_ABC_PROCESSED\"\n",
        "\n",
        "outbase = \"/content/abc_2048\"\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/gdrive/MyDrive/gpt2\")\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n",
        "text = \"\"\n",
        "songnum = 0\n",
        "pct = 0.0 # percent of songs to be used for validation\n",
        "\n",
        "files = os.listdir(indir)\n",
        "for song in files:\n",
        "\n",
        "    if song.split(\".\")[1] == \"abc\":\n",
        "\n",
        "        try:\n",
        "\n",
        "          # print(song)\n",
        "          fn = os.path.join(indir,song)\n",
        "\n",
        "          with open(fn,\"r\") as songfile:\n",
        "\n",
        "              data = songfile.read()\n",
        "\n",
        "              tokens = data.split(\" \")\n",
        "              numtokens = len(tokens)\n",
        "\n",
        "              suffix = \"eval\"\n",
        "              if random() > pct:\n",
        "                  suffix = \"train\"\n",
        "\n",
        "              outfile = outbase + \"_\" + suffix + \".txt\"\n",
        "\n",
        "              # make sure our songs are of a decent length\n",
        "              if numtokens < 1024: \n",
        "\n",
        "                  tokenized = tokenizer.encode(data, return_tensors='tf')\n",
        "                  print(len(tokenized[0]))\n",
        "\n",
        "                  if len(tokenized[0]) < 2048 and len(tokenized[0]) > 256:\n",
        "\n",
        "                    text = data + \"<|endoftext|>\\n\" # whitespace character helps training\n",
        "                    songnum += 1\n",
        "\n",
        "                    with open(outfile,\"a\") as f:\n",
        "                        f.writelines(text)\n",
        "                        text = \"\"\n",
        "        except:\n",
        "            print(\"probably a utf-8 error\")\n",
        "\n",
        "print(f\"Completions file contains {songnum} songs!\")"
      ],
      "metadata": {
        "id": "AraXGIccJjqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea700ad-a5fc-4836-9d9a-8e5faa7c0748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2798\n",
            "2400\n",
            "2119\n",
            "886\n",
            "890\n",
            "2886\n",
            "899\n",
            "2288\n",
            "2371\n",
            "1755\n",
            "1313\n",
            "2445\n",
            "2905\n",
            "933\n",
            "1934\n",
            "2028\n",
            "1296\n",
            "3164\n",
            "28365\n",
            "4616\n",
            "633\n",
            "3972\n",
            "1351\n",
            "2515\n",
            "3488\n",
            "1682\n",
            "920\n",
            "773\n",
            "2028\n",
            "1615\n",
            "3479\n",
            "247\n",
            "4125\n",
            "1900\n",
            "1297\n",
            "2818\n",
            "3058\n",
            "1313\n",
            "2754\n",
            "1378\n",
            "229\n",
            "2569\n",
            "1273\n",
            "6216\n",
            "2293\n",
            "1829\n",
            "642\n",
            "2902\n",
            "4665\n",
            "861\n",
            "864\n",
            "2047\n",
            "5924\n",
            "351\n",
            "3010\n",
            "650\n",
            "3310\n",
            "2295\n",
            "252\n",
            "1298\n",
            "3393\n",
            "1905\n",
            "773\n",
            "2085\n",
            "3538\n",
            "5963\n",
            "1573\n",
            "1386\n",
            "1394\n",
            "165\n",
            "179\n",
            "236\n",
            "4739\n",
            "867\n",
            "1613\n",
            "1109\n",
            "7286\n",
            "1058\n",
            "226\n",
            "1309\n",
            "1437\n",
            "2796\n",
            "2992\n",
            "3578\n",
            "905\n",
            "986\n",
            "1460\n",
            "157\n",
            "847\n",
            "9194\n",
            "1051\n",
            "1556\n",
            "6867\n",
            "1155\n",
            "3865\n",
            "2270\n",
            "1208\n",
            "2804\n",
            "2244\n",
            "339\n",
            "777\n",
            "1385\n",
            "2078\n",
            "2493\n",
            "4522\n",
            "1968\n",
            "3780\n",
            "1756\n",
            "1858\n",
            "1873\n",
            "2357\n",
            "2792\n",
            "560\n",
            "1699\n",
            "2368\n",
            "2606\n",
            "4426\n",
            "1698\n",
            "3803\n",
            "2722\n",
            "1411\n",
            "2412\n",
            "2328\n",
            "1050\n",
            "576\n",
            "537\n",
            "2498\n",
            "895\n",
            "1491\n",
            "410\n",
            "139\n",
            "234\n",
            "1946\n",
            "3860\n",
            "837\n",
            "1996\n",
            "392\n",
            "366\n",
            "332\n",
            "3897\n",
            "2448\n",
            "1084\n",
            "3823\n",
            "2795\n",
            "3290\n",
            "1066\n",
            "1457\n",
            "2013\n",
            "7730\n",
            "399\n",
            "3771\n",
            "3587\n",
            "1094\n",
            "2243\n",
            "2314\n",
            "2021\n",
            "2043\n",
            "320\n",
            "729\n",
            "1331\n",
            "953\n",
            "2462\n",
            "2253\n",
            "2915\n",
            "1930\n",
            "1471\n",
            "624\n",
            "389\n",
            "2325\n",
            "1850\n",
            "3334\n",
            "2570\n",
            "1666\n",
            "5330\n",
            "1843\n",
            "1881\n",
            "1698\n",
            "3843\n",
            "2703\n",
            "601\n",
            "1780\n",
            "1645\n",
            "3504\n",
            "2534\n",
            "1426\n",
            "3823\n",
            "479\n",
            "1938\n",
            "4531\n",
            "1025\n",
            "756\n",
            "1975\n",
            "264\n",
            "747\n",
            "1892\n",
            "1012\n",
            "1131\n",
            "2953\n",
            "1932\n",
            "1326\n",
            "1128\n",
            "2843\n",
            "357\n",
            "114\n",
            "369\n",
            "1410\n",
            "3141\n",
            "1571\n",
            "3611\n",
            "1192\n",
            "747\n",
            "1512\n",
            "4679\n",
            "1020\n",
            "1095\n",
            "2467\n",
            "452\n",
            "1003\n",
            "1705\n",
            "326\n",
            "543\n",
            "1668\n",
            "1250\n",
            "1699\n",
            "1248\n",
            "703\n",
            "1590\n",
            "1126\n",
            "731\n",
            "993\n",
            "3380\n",
            "1580\n",
            "1310\n",
            "191\n",
            "1129\n",
            "2450\n",
            "971\n",
            "1281\n",
            "2357\n",
            "1038\n",
            "3551\n",
            "3291\n",
            "1091\n",
            "1594\n",
            "689\n",
            "1956\n",
            "920\n",
            "4018\n",
            "426\n",
            "4558\n",
            "3014\n",
            "3823\n",
            "1718\n",
            "2718\n",
            "6380\n",
            "254\n",
            "1298\n",
            "1733\n",
            "3496\n",
            "406\n",
            "2521\n",
            "4962\n",
            "221\n",
            "4311\n",
            "3010\n",
            "6662\n",
            "3940\n",
            "6864\n",
            "761\n",
            "3805\n",
            "341\n",
            "5709\n",
            "3303\n",
            "3997\n",
            "1032\n",
            "3973\n",
            "1076\n",
            "294\n",
            "6207\n",
            "272\n",
            "3744\n",
            "229\n",
            "3490\n",
            "2855\n",
            "2633\n",
            "1795\n",
            "3340\n",
            "2470\n",
            "332\n",
            "3267\n",
            "1509\n",
            "5831\n",
            "3915\n",
            "3742\n",
            "2216\n",
            "4105\n",
            "850\n",
            "254\n",
            "7391\n",
            "3830\n",
            "2973\n",
            "2263\n",
            "942\n",
            "2653\n",
            "657\n",
            "2372\n",
            "921\n",
            "3058\n",
            "3353\n",
            "2289\n",
            "1635\n",
            "663\n",
            "2679\n",
            "5879\n",
            "2853\n",
            "1528\n",
            "4924\n",
            "2414\n",
            "1603\n",
            "228\n",
            "1965\n",
            "3276\n",
            "682\n",
            "1876\n",
            "874\n",
            "1999\n",
            "1159\n",
            "3568\n",
            "3181\n",
            "2057\n",
            "1181\n",
            "2839\n",
            "2226\n",
            "2809\n",
            "1811\n",
            "2763\n",
            "1488\n",
            "1724\n",
            "3997\n",
            "1460\n",
            "1718\n",
            "3151\n",
            "2248\n",
            "364\n",
            "447\n",
            "732\n",
            "540\n",
            "2153\n",
            "2313\n",
            "3292\n",
            "2029\n",
            "3458\n",
            "1462\n",
            "2248\n",
            "3028\n",
            "637\n",
            "4003\n",
            "1047\n",
            "2178\n",
            "3189\n",
            "2675\n",
            "303\n",
            "1269\n",
            "236\n",
            "3744\n",
            "2992\n",
            "495\n",
            "1585\n",
            "3110\n",
            "3107\n",
            "1323\n",
            "4776\n",
            "433\n",
            "5685\n",
            "2028\n",
            "4436\n",
            "674\n",
            "1679\n",
            "2679\n",
            "2021\n",
            "2055\n",
            "5189\n",
            "472\n",
            "1852\n",
            "3342\n",
            "1065\n",
            "778\n",
            "3307\n",
            "1307\n",
            "1250\n",
            "2715\n",
            "887\n",
            "1782\n",
            "886\n",
            "174\n",
            "135\n",
            "1913\n",
            "2789\n",
            "487\n",
            "1299\n",
            "1100\n",
            "508\n",
            "23499\n",
            "12961\n",
            "145\n",
            "4379\n",
            "2521\n",
            "2414\n",
            "4043\n",
            "3403\n",
            "1714\n",
            "2165\n",
            "2985\n",
            "3724\n",
            "3318\n",
            "1901\n",
            "255\n",
            "358\n",
            "1467\n",
            "3207\n",
            "3137\n",
            "1200\n",
            "7556\n",
            "1879\n",
            "2824\n",
            "1457\n",
            "1753\n",
            "4141\n",
            "2595\n",
            "2782\n",
            "1718\n",
            "773\n",
            "156\n",
            "739\n",
            "1228\n",
            "2273\n",
            "621\n",
            "270\n",
            "2517\n",
            "236\n",
            "523\n",
            "1673\n",
            "524\n",
            "3319\n",
            "805\n",
            "2358\n",
            "195\n",
            "1444\n",
            "2045\n",
            "1392\n",
            "2836\n",
            "323\n",
            "734\n",
            "2913\n",
            "6347\n",
            "1888\n",
            "659\n",
            "3635\n",
            "3107\n",
            "697\n",
            "907\n",
            "2070\n",
            "1162\n",
            "2245\n",
            "1605\n",
            "1100\n",
            "2747\n",
            "2199\n",
            "2523\n",
            "2013\n",
            "1947\n",
            "2116\n",
            "2685\n",
            "862\n",
            "4428\n",
            "2092\n",
            "2426\n",
            "1362\n",
            "1558\n",
            "2515\n",
            "6517\n",
            "2129\n",
            "3646\n",
            "4565\n",
            "4281\n",
            "849\n",
            "349\n",
            "423\n",
            "2391\n",
            "984\n",
            "374\n",
            "1373\n",
            "530\n",
            "870\n",
            "2125\n",
            "1221\n",
            "969\n",
            "2970\n",
            "423\n",
            "968\n",
            "353\n",
            "297\n",
            "882\n",
            "1335\n",
            "1552\n",
            "108\n",
            "842\n",
            "2287\n",
            "1995\n",
            "1996\n",
            "3408\n",
            "1020\n",
            "1711\n",
            "3544\n",
            "2475\n",
            "845\n",
            "515\n",
            "1563\n",
            "2868\n",
            "1307\n",
            "612\n",
            "2213\n",
            "1614\n",
            "2491\n",
            "1817\n",
            "2086\n",
            "3108\n",
            "1680\n",
            "457\n",
            "3264\n",
            "1556\n",
            "451\n",
            "5611\n",
            "1729\n",
            "1463\n",
            "2520\n",
            "2652\n",
            "2945\n",
            "2983\n",
            "556\n",
            "1516\n",
            "1633\n",
            "2671\n",
            "5163\n",
            "809\n",
            "1983\n",
            "196\n",
            "1791\n",
            "4209\n",
            "1166\n",
            "506\n",
            "551\n",
            "896\n",
            "1503\n",
            "2037\n",
            "408\n",
            "4710\n",
            "183\n",
            "2451\n",
            "4269\n",
            "4199\n",
            "676\n",
            "3273\n",
            "1674\n",
            "279\n",
            "1447\n",
            "380\n",
            "600\n",
            "494\n",
            "3026\n",
            "1084\n",
            "2769\n",
            "885\n",
            "2617\n",
            "2337\n",
            "2227\n",
            "2812\n",
            "729\n",
            "2538\n",
            "1679\n",
            "1156\n",
            "1167\n",
            "1308\n",
            "1470\n",
            "1597\n",
            "2201\n",
            "2496\n",
            "1984\n",
            "1039\n",
            "2873\n",
            "2737\n",
            "3573\n",
            "268\n",
            "1283\n",
            "3296\n",
            "6506\n",
            "2761\n",
            "334\n",
            "1958\n",
            "1225\n",
            "288\n",
            "12600\n",
            "1626\n",
            "605\n",
            "4484\n",
            "2975\n",
            "126\n",
            "2324\n",
            "2910\n",
            "828\n",
            "5405\n",
            "2262\n",
            "2902\n",
            "3240\n",
            "2821\n",
            "241\n",
            "3861\n",
            "1662\n",
            "2848\n",
            "878\n",
            "2874\n",
            "2917\n",
            "3980\n",
            "1512\n",
            "3866\n",
            "3140\n",
            "2064\n",
            "2955\n",
            "6259\n",
            "2377\n",
            "2715\n",
            "926\n",
            "2706\n",
            "439\n",
            "2308\n",
            "3001\n",
            "2741\n",
            "964\n",
            "2409\n",
            "2310\n",
            "3195\n",
            "1303\n",
            "4835\n",
            "2778\n",
            "2043\n",
            "216\n",
            "1937\n",
            "866\n",
            "1686\n",
            "181\n",
            "795\n",
            "420\n",
            "1853\n",
            "1701\n",
            "2071\n",
            "4237\n",
            "1070\n",
            "1351\n",
            "2430\n",
            "2647\n",
            "1177\n",
            "1177\n",
            "2489\n",
            "1669\n",
            "803\n",
            "1724\n",
            "2717\n",
            "459\n",
            "2716\n",
            "1938\n",
            "1467\n",
            "6679\n",
            "12608\n",
            "2824\n",
            "472\n",
            "4755\n",
            "4678\n",
            "4895\n",
            "2292\n",
            "1049\n",
            "1239\n",
            "1523\n",
            "1725\n",
            "1816\n",
            "398\n",
            "2194\n",
            "189\n",
            "2782\n",
            "3999\n",
            "1900\n",
            "2255\n",
            "1601\n",
            "981\n",
            "2217\n",
            "2551\n",
            "460\n",
            "1878\n",
            "6899\n",
            "4213\n",
            "1361\n",
            "636\n",
            "259\n",
            "376\n",
            "1485\n",
            "3330\n",
            "4992\n",
            "2391\n",
            "1612\n",
            "2090\n",
            "1051\n",
            "1122\n",
            "2493\n",
            "283\n",
            "2342\n",
            "3097\n",
            "116\n",
            "3716\n",
            "2648\n",
            "2040\n",
            "501\n",
            "2083\n",
            "1371\n",
            "1261\n",
            "1853\n",
            "838\n",
            "2484\n",
            "1598\n",
            "326\n",
            "2353\n",
            "4045\n",
            "3749\n",
            "1345\n",
            "1435\n",
            "717\n",
            "1415\n",
            "2303\n",
            "2258\n",
            "4548\n",
            "388\n",
            "1522\n",
            "907\n",
            "4252\n",
            "4828\n",
            "4246\n",
            "4235\n",
            "277\n",
            "952\n",
            "328\n",
            "1130\n",
            "1829\n",
            "3126\n",
            "1710\n",
            "2123\n",
            "121\n",
            "293\n",
            "272\n",
            "4277\n",
            "182\n",
            "2024\n",
            "1415\n",
            "286\n",
            "495\n",
            "2412\n",
            "2134\n",
            "2807\n",
            "780\n",
            "1686\n",
            "7485\n",
            "2339\n",
            "2514\n",
            "2220\n",
            "965\n",
            "1619\n",
            "2434\n",
            "2673\n",
            "1026\n",
            "427\n",
            "1217\n",
            "410\n",
            "438\n",
            "718\n",
            "667\n",
            "7041\n",
            "2538\n",
            "1449\n",
            "609\n",
            "394\n",
            "1104\n",
            "2697\n",
            "3559\n",
            "174\n",
            "3687\n",
            "1255\n",
            "2612\n",
            "1378\n",
            "986\n",
            "431\n",
            "2887\n",
            "3500\n",
            "685\n",
            "1462\n",
            "474\n",
            "1600\n",
            "310\n",
            "350\n",
            "2649\n",
            "373\n",
            "821\n",
            "127\n",
            "198\n",
            "221\n",
            "180\n",
            "2526\n",
            "660\n",
            "279\n",
            "265\n",
            "1153\n",
            "1820\n",
            "270\n",
            "1022\n",
            "5514\n",
            "1205\n",
            "2199\n",
            "333\n",
            "348\n",
            "1367\n",
            "2442\n",
            "386\n",
            "1335\n",
            "189\n",
            "4839\n",
            "1047\n",
            "1456\n",
            "1749\n",
            "750\n",
            "3486\n",
            "2745\n",
            "1151\n",
            "1301\n",
            "2475\n",
            "2418\n",
            "2064\n",
            "3298\n",
            "2445\n",
            "2419\n",
            "2742\n",
            "2102\n",
            "2851\n",
            "1028\n",
            "951\n",
            "2288\n",
            "1168\n",
            "3676\n",
            "2782\n",
            "853\n",
            "1477\n",
            "1342\n",
            "1584\n",
            "1653\n",
            "847\n",
            "1224\n",
            "1615\n",
            "2459\n",
            "823\n",
            "4938\n",
            "1751\n",
            "3599\n",
            "1435\n",
            "2128\n",
            "703\n",
            "4650\n",
            "3078\n",
            "1254\n",
            "1643\n",
            "2343\n",
            "1545\n",
            "2022\n",
            "1505\n",
            "4305\n",
            "1535\n",
            "2073\n",
            "2295\n",
            "1419\n",
            "1040\n",
            "3869\n",
            "2835\n",
            "973\n",
            "1299\n",
            "3595\n",
            "1754\n",
            "1487\n",
            "2163\n",
            "2238\n",
            "1384\n",
            "1997\n",
            "1207\n",
            "3225\n",
            "881\n",
            "5024\n",
            "2196\n",
            "3795\n",
            "1238\n",
            "921\n",
            "981\n",
            "628\n",
            "2047\n",
            "957\n",
            "4107\n",
            "2406\n",
            "1650\n",
            "2337\n",
            "928\n",
            "1657\n",
            "3458\n",
            "756\n",
            "636\n",
            "1802\n",
            "2397\n",
            "2617\n",
            "2638\n",
            "3329\n",
            "2106\n",
            "664\n",
            "3046\n",
            "1126\n",
            "1653\n",
            "2140\n",
            "405\n",
            "3900\n",
            "1838\n",
            "1695\n",
            "1666\n",
            "1923\n",
            "945\n",
            "518\n",
            "1770\n",
            "1893\n",
            "5436\n",
            "3073\n",
            "2291\n",
            "812\n",
            "2954\n",
            "891\n",
            "143\n",
            "4994\n",
            "4480\n",
            "1157\n",
            "1383\n",
            "2350\n",
            "1213\n",
            "251\n",
            "338\n",
            "216\n",
            "220\n",
            "2391\n",
            "1837\n",
            "2007\n",
            "1102\n",
            "1238\n",
            "1638\n",
            "2950\n",
            "1122\n",
            "802\n",
            "1150\n",
            "1829\n",
            "827\n",
            "363\n",
            "2054\n",
            "1575\n",
            "761\n",
            "3531\n",
            "3339\n",
            "3381\n",
            "1273\n",
            "2125\n",
            "693\n",
            "1055\n",
            "563\n",
            "1483\n",
            "1711\n",
            "1530\n",
            "963\n",
            "3607\n",
            "769\n",
            "547\n",
            "177\n",
            "1909\n",
            "652\n",
            "201\n",
            "1085\n",
            "883\n",
            "2427\n",
            "3068\n",
            "247\n",
            "248\n",
            "2569\n",
            "4296\n",
            "982\n",
            "2533\n",
            "2328\n",
            "1982\n",
            "3296\n",
            "1865\n",
            "2300\n",
            "1322\n",
            "5009\n",
            "2423\n",
            "1095\n",
            "2368\n",
            "840\n",
            "565\n",
            "519\n",
            "3316\n",
            "632\n",
            "2150\n",
            "618\n",
            "1895\n",
            "2306\n",
            "3426\n",
            "3589\n",
            "1912\n",
            "3535\n",
            "2823\n",
            "574\n",
            "1345\n",
            "3165\n",
            "1408\n",
            "4079\n",
            "2200\n",
            "2206\n",
            "1923\n",
            "1718\n",
            "645\n",
            "3752\n",
            "1830\n",
            "3941\n",
            "739\n",
            "1097\n",
            "979\n",
            "2252\n",
            "2267\n",
            "1976\n",
            "802\n",
            "297\n",
            "500\n",
            "592\n",
            "2022\n",
            "1858\n",
            "727\n",
            "4330\n",
            "1330\n",
            "2863\n",
            "4417\n",
            "3167\n",
            "1358\n",
            "3563\n",
            "1432\n",
            "3855\n",
            "1307\n",
            "449\n",
            "668\n",
            "1044\n",
            "1841\n",
            "1367\n",
            "918\n",
            "534\n",
            "1967\n",
            "4011\n",
            "3848\n",
            "664\n",
            "2114\n",
            "1448\n",
            "1002\n",
            "2102\n",
            "3629\n",
            "655\n",
            "1495\n",
            "781\n",
            "1134\n",
            "2926\n",
            "3030\n",
            "494\n",
            "2313\n",
            "3034\n",
            "2538\n",
            "3534\n",
            "243\n",
            "4196\n",
            "2185\n",
            "2243\n",
            "1103\n",
            "898\n",
            "1423\n",
            "1854\n",
            "2798\n",
            "1975\n",
            "550\n",
            "1275\n",
            "2712\n",
            "2660\n",
            "2738\n",
            "2122\n",
            "1356\n",
            "1491\n",
            "1406\n",
            "1601\n",
            "3117\n",
            "1452\n",
            "1175\n",
            "1808\n",
            "1140\n",
            "1878\n",
            "683\n",
            "222\n",
            "4153\n",
            "4832\n",
            "151\n",
            "2555\n",
            "3540\n",
            "2270\n",
            "842\n",
            "2518\n",
            "2612\n",
            "97\n",
            "3900\n",
            "933\n",
            "1453\n",
            "1779\n",
            "1344\n",
            "1872\n",
            "3052\n",
            "1889\n",
            "2772\n",
            "2621\n",
            "10011\n",
            "1536\n",
            "1306\n",
            "1471\n",
            "1937\n",
            "2585\n",
            "2633\n",
            "3715\n",
            "2786\n",
            "1620\n",
            "3002\n",
            "1774\n",
            "727\n",
            "3429\n",
            "1909\n",
            "1909\n",
            "636\n",
            "503\n",
            "331\n",
            "659\n",
            "228\n",
            "571\n",
            "267\n",
            "508\n",
            "113\n",
            "376\n",
            "1162\n",
            "1763\n",
            "577\n",
            "251\n",
            "834\n",
            "1542\n",
            "1532\n",
            "537\n",
            "1361\n",
            "772\n",
            "202\n",
            "5133\n",
            "4049\n",
            "2017\n",
            "2283\n",
            "565\n",
            "1773\n",
            "2084\n",
            "3309\n",
            "2202\n",
            "1605\n",
            "568\n",
            "1025\n",
            "1201\n",
            "865\n",
            "535\n",
            "5073\n",
            "1706\n",
            "1945\n",
            "2460\n",
            "5236\n",
            "3175\n",
            "3755\n",
            "3817\n",
            "2501\n",
            "714\n",
            "1438\n",
            "607\n",
            "2392\n",
            "3358\n",
            "422\n",
            "683\n",
            "2374\n",
            "3181\n",
            "1088\n",
            "206\n",
            "1069\n",
            "3259\n",
            "2242\n",
            "2744\n",
            "831\n",
            "1758\n",
            "1688\n",
            "3026\n",
            "2607\n",
            "4702\n",
            "2311\n",
            "871\n",
            "297\n",
            "4311\n",
            "1509\n",
            "4027\n",
            "2310\n",
            "849\n",
            "1234\n",
            "1948\n",
            "2855\n",
            "495\n",
            "2101\n",
            "2724\n",
            "1842\n",
            "1435\n",
            "1711\n",
            "2228\n",
            "2473\n",
            "1996\n",
            "2250\n",
            "3886\n",
            "3820\n",
            "5451\n",
            "3136\n",
            "1792\n",
            "612\n",
            "2074\n",
            "1417\n",
            "987\n",
            "2502\n",
            "3619\n",
            "3343\n",
            "251\n",
            "1749\n",
            "1426\n",
            "2228\n",
            "1656\n",
            "2752\n",
            "2020\n",
            "531\n",
            "1667\n",
            "355\n",
            "4832\n",
            "291\n",
            "867\n",
            "998\n",
            "608\n",
            "1550\n",
            "961\n",
            "2139\n",
            "1902\n",
            "591\n",
            "728\n",
            "2001\n",
            "1315\n",
            "2047\n",
            "574\n",
            "2699\n",
            "245\n",
            "301\n",
            "1356\n",
            "1117\n",
            "240\n",
            "199\n",
            "220\n",
            "1755\n",
            "571\n",
            "496\n",
            "1153\n",
            "1998\n",
            "1916\n",
            "1569\n",
            "518\n",
            "237\n",
            "277\n",
            "286\n",
            "4029\n",
            "824\n",
            "711\n",
            "1276\n",
            "575\n",
            "426\n",
            "261\n",
            "1025\n",
            "2906\n",
            "419\n",
            "3310\n",
            "909\n",
            "675\n",
            "1696\n",
            "1768\n",
            "691\n",
            "1650\n",
            "226\n",
            "2951\n",
            "1654\n",
            "2152\n",
            "1195\n",
            "1638\n",
            "755\n",
            "2003\n",
            "738\n",
            "2703\n",
            "999\n",
            "3457\n",
            "2828\n",
            "2420\n",
            "1704\n",
            "360\n",
            "2170\n",
            "1259\n",
            "1941\n",
            "237\n",
            "1540\n",
            "495\n",
            "2250\n",
            "1652\n",
            "1663\n",
            "2137\n",
            "2536\n",
            "2393\n",
            "3441\n",
            "2287\n",
            "2580\n",
            "2422\n",
            "1346\n",
            "1873\n",
            "1995\n",
            "3115\n",
            "1843\n",
            "3162\n",
            "2366\n",
            "2083\n",
            "2553\n",
            "3021\n",
            "2518\n",
            "1407\n",
            "2698\n",
            "438\n",
            "959\n",
            "504\n",
            "2860\n",
            "250\n",
            "1384\n",
            "1367\n",
            "1584\n",
            "1285\n",
            "3884\n",
            "1075\n",
            "616\n",
            "2115\n",
            "576\n",
            "707\n",
            "515\n",
            "1412\n",
            "3699\n",
            "1507\n",
            "1149\n",
            "1512\n",
            "3103\n",
            "924\n",
            "220\n",
            "209\n",
            "238\n",
            "2862\n",
            "1928\n",
            "3611\n",
            "3898\n",
            "4803\n",
            "218\n",
            "237\n",
            "921\n",
            "1137\n",
            "6501\n",
            "3197\n",
            "1452\n",
            "401\n",
            "430\n",
            "517\n",
            "2073\n",
            "1517\n",
            "207\n",
            "1162\n",
            "685\n",
            "1007\n",
            "1501\n",
            "1911\n",
            "1824\n",
            "2130\n",
            "836\n",
            "885\n",
            "2414\n",
            "267\n",
            "9892\n",
            "2046\n",
            "309\n",
            "4745\n",
            "4337\n",
            "2709\n",
            "2069\n",
            "3574\n",
            "2052\n",
            "4410\n",
            "630\n",
            "2307\n",
            "3000\n",
            "2346\n",
            "1651\n",
            "2366\n",
            "1492\n",
            "2167\n",
            "2087\n",
            "4254\n",
            "1361\n",
            "9553\n",
            "1995\n",
            "3356\n",
            "613\n",
            "612\n",
            "719\n",
            "806\n",
            "740\n",
            "2019\n",
            "1314\n",
            "769\n",
            "754\n",
            "2832\n",
            "798\n",
            "2199\n",
            "547\n",
            "1104\n",
            "5336\n",
            "1167\n",
            "2154\n",
            "2692\n",
            "496\n",
            "1131\n",
            "785\n",
            "5565\n",
            "2526\n",
            "741\n",
            "1536\n",
            "156\n",
            "116\n",
            "2656\n",
            "1881\n",
            "1802\n",
            "124\n",
            "604\n",
            "3824\n",
            "2317\n",
            "1536\n",
            "1810\n",
            "751\n",
            "1917\n",
            "1230\n",
            "4550\n",
            "793\n",
            "2538\n",
            "2150\n",
            "2697\n",
            "2484\n",
            "2093\n",
            "1763\n",
            "389\n",
            "1843\n",
            "3320\n",
            "1397\n",
            "807\n",
            "1352\n",
            "2262\n",
            "1485\n",
            "474\n",
            "1076\n",
            "1638\n",
            "1320\n",
            "860\n",
            "911\n",
            "3179\n",
            "2483\n",
            "408\n",
            "94\n",
            "605\n",
            "696\n",
            "736\n",
            "578\n",
            "536\n",
            "2663\n",
            "996\n",
            "2693\n",
            "2109\n",
            "2530\n",
            "3011\n",
            "293\n",
            "854\n",
            "3177\n",
            "1582\n",
            "2348\n",
            "863\n",
            "1765\n",
            "1803\n",
            "3453\n",
            "2970\n",
            "533\n",
            "988\n",
            "3598\n",
            "2647\n",
            "1780\n",
            "2232\n",
            "546\n",
            "2746\n",
            "5958\n",
            "2314\n",
            "2580\n",
            "3399\n",
            "2727\n",
            "4438\n",
            "3027\n",
            "2344\n",
            "3683\n",
            "1679\n",
            "706\n",
            "2821\n",
            "449\n",
            "4295\n",
            "455\n",
            "3440\n",
            "2528\n",
            "1580\n",
            "1419\n",
            "1350\n",
            "869\n",
            "2232\n",
            "1332\n",
            "778\n",
            "1227\n",
            "608\n",
            "1930\n",
            "1087\n",
            "492\n",
            "3326\n",
            "3185\n",
            "2194\n",
            "1081\n",
            "870\n",
            "3004\n",
            "2724\n",
            "833\n",
            "2177\n",
            "588\n",
            "2155\n",
            "993\n",
            "866\n",
            "2177\n",
            "2789\n",
            "2911\n",
            "2239\n",
            "1036\n",
            "3656\n",
            "1376\n",
            "3146\n",
            "603\n",
            "2384\n",
            "132\n",
            "767\n",
            "3683\n",
            "2900\n",
            "1093\n",
            "1526\n",
            "3043\n",
            "125\n",
            "1789\n",
            "1801\n",
            "5629\n",
            "3272\n",
            "2184\n",
            "3277\n",
            "2021\n",
            "4370\n",
            "1752\n",
            "1983\n",
            "2616\n",
            "2587\n",
            "1805\n",
            "1244\n",
            "1822\n",
            "260\n",
            "405\n",
            "2723\n",
            "891\n",
            "961\n",
            "908\n",
            "1324\n",
            "1050\n",
            "2098\n",
            "132\n",
            "2812\n",
            "112\n",
            "903\n",
            "2930\n",
            "581\n",
            "2607\n",
            "2128\n",
            "862\n",
            "2613\n",
            "1945\n",
            "1608\n",
            "2035\n",
            "2672\n",
            "1317\n",
            "1102\n",
            "240\n",
            "2163\n",
            "445\n",
            "3809\n",
            "2218\n",
            "594\n",
            "5303\n",
            "4564\n",
            "4118\n",
            "4007\n",
            "2707\n",
            "3456\n",
            "4083\n",
            "2916\n",
            "1215\n",
            "487\n",
            "7341\n",
            "971\n",
            "3709\n",
            "1605\n",
            "4648\n",
            "1969\n",
            "3058\n",
            "905\n",
            "2477\n",
            "460\n",
            "1723\n",
            "3603\n",
            "4283\n",
            "3708\n",
            "1236\n",
            "1834\n",
            "1967\n",
            "1060\n",
            "1588\n",
            "3826\n",
            "1758\n",
            "2400\n",
            "1136\n",
            "2424\n",
            "814\n",
            "628\n",
            "504\n",
            "450\n",
            "1957\n",
            "1734\n",
            "1386\n",
            "2166\n",
            "1506\n",
            "2836\n",
            "503\n",
            "1065\n",
            "1765\n",
            "436\n",
            "904\n",
            "1922\n",
            "1381\n",
            "2424\n",
            "2214\n",
            "2759\n",
            "281\n",
            "848\n",
            "2048\n",
            "4316\n",
            "765\n",
            "2241\n",
            "1671\n",
            "1853\n",
            "4177\n",
            "2463\n",
            "243\n",
            "2737\n",
            "2820\n",
            "3615\n",
            "2344\n",
            "982\n",
            "192\n",
            "1193\n",
            "1931\n",
            "2151\n",
            "3258\n",
            "2980\n",
            "1217\n",
            "3610\n",
            "3396\n",
            "1595\n",
            "1816\n",
            "1053\n",
            "255\n",
            "1195\n",
            "843\n",
            "1124\n",
            "494\n",
            "1909\n",
            "340\n",
            "1971\n",
            "5350\n",
            "2180\n",
            "555\n",
            "1041\n",
            "1437\n",
            "3763\n",
            "1334\n",
            "2109\n",
            "1943\n",
            "2223\n",
            "2801\n",
            "605\n",
            "4266\n",
            "1321\n",
            "3159\n",
            "1582\n",
            "1939\n",
            "1848\n",
            "5509\n",
            "2889\n",
            "2943\n",
            "2257\n",
            "1144\n",
            "517\n",
            "3043\n",
            "3667\n",
            "3761\n",
            "2960\n",
            "3433\n",
            "2662\n",
            "2456\n",
            "606\n",
            "2192\n",
            "2121\n",
            "1810\n",
            "299\n",
            "2988\n",
            "283\n",
            "165\n",
            "190\n",
            "238\n",
            "930\n",
            "384\n",
            "478\n",
            "3862\n",
            "2061\n",
            "1807\n",
            "914\n",
            "2847\n",
            "1753\n",
            "1829\n",
            "2997\n",
            "5763\n",
            "1801\n",
            "1400\n",
            "431\n",
            "1547\n",
            "2314\n",
            "2963\n",
            "2753\n",
            "8497\n",
            "4650\n",
            "3746\n",
            "5106\n",
            "1499\n",
            "880\n",
            "342\n",
            "2124\n",
            "1387\n",
            "910\n",
            "656\n",
            "2655\n",
            "3794\n",
            "5119\n",
            "2739\n",
            "1299\n",
            "1682\n",
            "3747\n",
            "1181\n",
            "1276\n",
            "1115\n",
            "2083\n",
            "3233\n",
            "351\n",
            "1510\n",
            "1041\n",
            "6489\n",
            "648\n",
            "3729\n",
            "2404\n",
            "1389\n",
            "132\n",
            "653\n",
            "1055\n",
            "3580\n",
            "2042\n",
            "1079\n",
            "520\n",
            "1086\n",
            "1819\n",
            "809\n",
            "3854\n",
            "2810\n",
            "2863\n",
            "3538\n",
            "4153\n",
            "5270\n",
            "3833\n",
            "1021\n",
            "307\n",
            "9324\n",
            "873\n",
            "1235\n",
            "714\n",
            "1896\n",
            "327\n",
            "3967\n",
            "820\n",
            "178\n",
            "232\n",
            "220\n",
            "675\n",
            "1057\n",
            "3750\n",
            "946\n",
            "2495\n",
            "9716\n",
            "971\n",
            "3303\n",
            "4411\n",
            "1216\n",
            "256\n",
            "2251\n",
            "3116\n",
            "402\n",
            "3308\n",
            "1971\n",
            "2898\n",
            "2386\n",
            "3124\n",
            "1461\n",
            "3337\n",
            "2093\n",
            "1940\n",
            "427\n",
            "2639\n",
            "4287\n",
            "3058\n",
            "2204\n",
            "1362\n",
            "2447\n",
            "1763\n",
            "1969\n",
            "0\n",
            "806\n",
            "4631\n",
            "5653\n",
            "3308\n",
            "2239\n",
            "6909\n",
            "1719\n",
            "3009\n",
            "2799\n",
            "4120\n",
            "116\n",
            "3356\n",
            "2754\n",
            "2507\n",
            "2165\n",
            "3581\n",
            "241\n",
            "272\n",
            "1717\n",
            "202\n",
            "2949\n",
            "344\n",
            "511\n",
            "1784\n",
            "230\n",
            "688\n",
            "473\n",
            "2630\n",
            "120\n",
            "1152\n",
            "1955\n",
            "939\n",
            "2237\n",
            "882\n",
            "3042\n",
            "678\n",
            "2120\n",
            "1605\n",
            "2184\n",
            "2470\n",
            "5914\n",
            "1040\n",
            "2668\n",
            "307\n",
            "1636\n",
            "1908\n",
            "1861\n",
            "1086\n",
            "274\n",
            "2905\n",
            "523\n",
            "2452\n",
            "1888\n",
            "3238\n",
            "2502\n",
            "416\n",
            "3085\n",
            "7031\n",
            "7931\n",
            "269\n",
            "8338\n",
            "329\n",
            "461\n",
            "1893\n",
            "948\n",
            "724\n",
            "2477\n",
            "6203\n",
            "5302\n",
            "3065\n",
            "4802\n",
            "1946\n",
            "3827\n",
            "3069\n",
            "1229\n",
            "4677\n",
            "969\n",
            "412\n",
            "3462\n",
            "5615\n",
            "156\n",
            "2549\n",
            "2421\n",
            "3539\n",
            "1400\n",
            "798\n",
            "875\n",
            "1050\n",
            "3316\n",
            "2642\n",
            "2649\n",
            "2385\n",
            "1040\n",
            "3189\n",
            "1714\n",
            "797\n",
            "1575\n",
            "880\n",
            "2327\n",
            "1719\n",
            "1446\n",
            "2662\n",
            "3237\n",
            "292\n",
            "1151\n",
            "4386\n",
            "302\n",
            "161\n",
            "436\n",
            "829\n",
            "1032\n",
            "1255\n",
            "4310\n",
            "1169\n",
            "3262\n",
            "2925\n",
            "3230\n",
            "268\n",
            "1296\n",
            "2525\n",
            "1176\n",
            "1322\n",
            "325\n",
            "331\n",
            "2347\n",
            "2328\n",
            "552\n",
            "2516\n",
            "1044\n",
            "1597\n",
            "2092\n",
            "1703\n",
            "1474\n",
            "3283\n",
            "5151\n",
            "1598\n",
            "2102\n",
            "647\n",
            "724\n",
            "1731\n",
            "2476\n",
            "1093\n",
            "568\n",
            "598\n",
            "948\n",
            "3441\n",
            "4061\n",
            "1636\n",
            "1633\n",
            "584\n",
            "1930\n",
            "2236\n",
            "2285\n",
            "4220\n",
            "1701\n",
            "2355\n",
            "1701\n",
            "3359\n",
            "3566\n",
            "2092\n",
            "3167\n",
            "3667\n",
            "296\n",
            "7557\n",
            "2812\n",
            "3169\n",
            "3995\n",
            "1661\n",
            "2162\n",
            "491\n",
            "354\n",
            "1775\n",
            "222\n",
            "1985\n",
            "3122\n",
            "1279\n",
            "934\n",
            "2992\n",
            "1969\n",
            "766\n",
            "486\n",
            "3062\n",
            "3799\n",
            "1762\n",
            "3888\n",
            "332\n",
            "1262\n",
            "2900\n",
            "5273\n",
            "3996\n",
            "321\n",
            "856\n",
            "2659\n",
            "1157\n",
            "2147\n",
            "1261\n",
            "305\n",
            "6615\n",
            "3041\n",
            "535\n",
            "289\n",
            "661\n",
            "1996\n",
            "1382\n",
            "909\n",
            "2702\n",
            "1799\n",
            "1350\n",
            "518\n",
            "731\n",
            "602\n",
            "1483\n",
            "3124\n",
            "1182\n",
            "807\n",
            "1071\n",
            "3262\n",
            "3100\n",
            "480\n",
            "1089\n",
            "1626\n",
            "1954\n",
            "376\n",
            "465\n",
            "4562\n",
            "3125\n",
            "1599\n",
            "1513\n",
            "965\n",
            "577\n",
            "445\n",
            "1588\n",
            "3277\n",
            "2011\n",
            "4006\n",
            "1462\n",
            "264\n",
            "1423\n",
            "1010\n",
            "2170\n",
            "3458\n",
            "748\n",
            "8832\n",
            "157\n",
            "317\n",
            "368\n",
            "1259\n",
            "2574\n",
            "3982\n",
            "7233\n",
            "268\n",
            "339\n",
            "2715\n",
            "1886\n",
            "2761\n",
            "2383\n",
            "772\n",
            "3046\n",
            "1279\n",
            "1876\n",
            "2309\n",
            "1119\n",
            "3039\n",
            "166\n",
            "3668\n",
            "1481\n",
            "2599\n",
            "495\n",
            "867\n",
            "2251\n",
            "1330\n",
            "408\n",
            "923\n",
            "1709\n",
            "1800\n",
            "1702\n",
            "4135\n",
            "2463\n",
            "121\n",
            "3789\n",
            "2068\n",
            "2552\n",
            "169\n",
            "3195\n",
            "441\n",
            "120\n",
            "165\n",
            "3958\n",
            "147\n",
            "1605\n",
            "2348\n",
            "2149\n",
            "3512\n",
            "1566\n",
            "2647\n",
            "641\n",
            "4373\n",
            "38262\n",
            "461\n",
            "3503\n",
            "2962\n",
            "4151\n",
            "400\n",
            "2834\n",
            "3295\n",
            "2128\n",
            "314\n",
            "2882\n",
            "1183\n",
            "1794\n",
            "649\n",
            "207\n",
            "2426\n",
            "1106\n",
            "2672\n",
            "2156\n",
            "1290\n",
            "2001\n",
            "1429\n",
            "803\n",
            "1932\n",
            "1732\n",
            "1688\n",
            "118\n",
            "260\n",
            "403\n",
            "5647\n",
            "1590\n",
            "1464\n",
            "2872\n",
            "534\n",
            "1627\n",
            "7240\n",
            "111\n",
            "2244\n",
            "2908\n",
            "5262\n",
            "844\n",
            "1954\n",
            "2373\n",
            "5091\n",
            "3765\n",
            "1724\n",
            "2032\n",
            "2407\n",
            "3020\n",
            "216\n",
            "3370\n",
            "1032\n",
            "2370\n",
            "1548\n",
            "4507\n",
            "761\n",
            "4526\n",
            "1181\n",
            "338\n",
            "953\n",
            "2312\n",
            "238\n",
            "1217\n",
            "1054\n",
            "790\n",
            "1391\n",
            "1163\n",
            "316\n",
            "415\n",
            "1095\n",
            "300\n",
            "372\n",
            "1083\n",
            "1556\n",
            "1423\n",
            "3237\n",
            "1231\n",
            "2562\n",
            "2604\n",
            "219\n",
            "897\n",
            "275\n",
            "1360\n",
            "2341\n",
            "2843\n",
            "2308\n",
            "365\n",
            "547\n",
            "1890\n",
            "4707\n",
            "1553\n",
            "3679\n",
            "205\n",
            "250\n",
            "217\n",
            "2359\n",
            "1006\n",
            "313\n",
            "1644\n",
            "411\n",
            "997\n",
            "1284\n",
            "219\n",
            "1469\n",
            "808\n",
            "2321\n",
            "3343\n",
            "2397\n",
            "322\n",
            "250\n",
            "375\n",
            "1733\n",
            "1850\n",
            "1284\n",
            "1700\n",
            "1443\n",
            "2593\n",
            "2412\n",
            "242\n",
            "2958\n",
            "481\n",
            "1625\n",
            "455\n",
            "6106\n",
            "1821\n",
            "2803\n",
            "3040\n",
            "1905\n",
            "519\n",
            "2220\n",
            "4021\n",
            "4644\n",
            "3822\n",
            "1087\n",
            "542\n",
            "546\n",
            "3562\n",
            "3101\n",
            "702\n",
            "3274\n",
            "222\n",
            "852\n",
            "2182\n",
            "2075\n",
            "349\n",
            "1589\n",
            "1622\n",
            "915\n",
            "1118\n",
            "1917\n",
            "3236\n",
            "262\n",
            "613\n",
            "2103\n",
            "557\n",
            "1314\n",
            "1572\n",
            "1696\n",
            "1075\n",
            "4485\n",
            "2375\n",
            "285\n",
            "1922\n",
            "3600\n",
            "341\n",
            "3328\n",
            "4859\n",
            "960\n",
            "584\n",
            "1563\n",
            "2434\n",
            "1371\n",
            "1719\n",
            "807\n",
            "2548\n",
            "4059\n",
            "2235\n",
            "2514\n",
            "2200\n",
            "312\n",
            "2592\n",
            "659\n",
            "11150\n",
            "2027\n",
            "1874\n",
            "554\n",
            "200\n",
            "3741\n",
            "3155\n",
            "4336\n",
            "543\n",
            "396\n",
            "1931\n",
            "260\n",
            "1458\n",
            "700\n",
            "2894\n",
            "3723\n",
            "1543\n",
            "911\n",
            "2453\n",
            "1268\n",
            "4982\n",
            "3602\n",
            "647\n",
            "289\n",
            "1102\n",
            "2505\n",
            "1997\n",
            "1310\n",
            "1012\n",
            "1558\n",
            "1332\n",
            "3628\n",
            "525\n",
            "2982\n",
            "1126\n",
            "1123\n",
            "441\n",
            "1645\n",
            "6556\n",
            "3772\n",
            "2837\n",
            "2081\n",
            "1834\n",
            "2331\n",
            "557\n",
            "2316\n",
            "1189\n",
            "2504\n",
            "6875\n",
            "3609\n",
            "3677\n",
            "2896\n",
            "3710\n",
            "96\n",
            "763\n",
            "1615\n",
            "4990\n",
            "1217\n",
            "1832\n",
            "3961\n",
            "620\n",
            "2160\n",
            "1076\n",
            "634\n",
            "3077\n",
            "1035\n",
            "565\n",
            "3381\n",
            "234\n",
            "338\n",
            "3483\n",
            "342\n",
            "3135\n",
            "1806\n",
            "1060\n",
            "2292\n",
            "2101\n",
            "4178\n",
            "1838\n",
            "5980\n",
            "1193\n",
            "2454\n",
            "3067\n",
            "2536\n",
            "4279\n",
            "957\n",
            "1378\n",
            "137\n",
            "259\n",
            "2286\n",
            "795\n",
            "2689\n",
            "2188\n",
            "245\n",
            "1046\n",
            "1304\n",
            "92\n",
            "661\n",
            "839\n",
            "3162\n",
            "1507\n",
            "132\n",
            "3843\n",
            "2579\n",
            "2793\n",
            "1537\n",
            "4101\n",
            "92\n",
            "2255\n",
            "999\n",
            "3080\n",
            "1586\n",
            "1109\n",
            "3643\n",
            "1473\n",
            "1048\n",
            "2810\n",
            "897\n",
            "3976\n",
            "1041\n",
            "132\n",
            "3032\n",
            "2560\n",
            "0\n",
            "1380\n",
            "2954\n",
            "906\n",
            "6008\n",
            "3178\n",
            "6854\n",
            "807\n",
            "3073\n",
            "2698\n",
            "124\n",
            "1760\n",
            "451\n",
            "727\n",
            "721\n",
            "134\n",
            "2641\n",
            "3676\n",
            "4302\n",
            "2560\n",
            "1560\n",
            "1931\n",
            "774\n",
            "2142\n",
            "2522\n",
            "842\n",
            "5395\n",
            "2337\n",
            "3909\n",
            "189\n",
            "74\n",
            "2691\n",
            "1038\n",
            "1236\n",
            "166\n",
            "1713\n",
            "1677\n",
            "443\n",
            "169\n",
            "2345\n",
            "639\n",
            "1812\n",
            "2200\n",
            "1268\n",
            "3293\n",
            "2734\n",
            "591\n",
            "4533\n",
            "2189\n",
            "1735\n",
            "3257\n",
            "1824\n",
            "1924\n",
            "1633\n",
            "1751\n",
            "1958\n",
            "1417\n",
            "985\n",
            "2033\n",
            "2641\n",
            "4944\n",
            "2540\n",
            "2748\n",
            "1741\n",
            "2961\n",
            "422\n",
            "1620\n",
            "1051\n",
            "1717\n",
            "3378\n",
            "438\n",
            "643\n",
            "2428\n",
            "1092\n",
            "2646\n",
            "354\n",
            "1631\n",
            "3246\n",
            "325\n",
            "574\n",
            "3040\n",
            "3787\n",
            "618\n",
            "2975\n",
            "1121\n",
            "1833\n",
            "123\n",
            "3003\n",
            "1040\n",
            "2173\n",
            "903\n",
            "835\n",
            "2948\n",
            "2022\n",
            "6377\n",
            "1087\n",
            "76\n",
            "2303\n",
            "1040\n",
            "0\n",
            "3677\n",
            "1681\n",
            "616\n",
            "1368\n",
            "1452\n",
            "1894\n",
            "161\n",
            "2938\n",
            "881\n",
            "509\n",
            "1501\n",
            "1597\n",
            "3292\n",
            "915\n",
            "563\n",
            "161\n",
            "307\n",
            "2642\n",
            "631\n",
            "2787\n",
            "147\n",
            "1419\n",
            "2430\n",
            "1207\n",
            "682\n",
            "2663\n",
            "339\n",
            "2798\n",
            "2830\n",
            "3436\n",
            "2569\n",
            "284\n",
            "1776\n",
            "853\n",
            "2078\n",
            "3090\n",
            "1790\n",
            "1480\n",
            "2637\n",
            "1914\n",
            "331\n",
            "621\n",
            "159\n",
            "4070\n",
            "1512\n",
            "848\n",
            "3248\n",
            "598\n",
            "3036\n",
            "155\n",
            "972\n",
            "2786\n",
            "127\n",
            "2262\n",
            "70\n",
            "1931\n",
            "3023\n",
            "544\n",
            "160\n",
            "851\n",
            "1773\n",
            "181\n",
            "384\n",
            "2886\n",
            "2249\n",
            "206\n",
            "252\n",
            "1826\n",
            "1250\n",
            "942\n",
            "1272\n",
            "1332\n",
            "3021\n",
            "416\n",
            "621\n",
            "1511\n",
            "1020\n",
            "2303\n",
            "3133\n",
            "4446\n",
            "1963\n",
            "1802\n",
            "1130\n",
            "1765\n",
            "1167\n",
            "738\n",
            "1978\n",
            "2268\n",
            "2436\n",
            "429\n",
            "218\n",
            "523\n",
            "4173\n",
            "3467\n",
            "759\n",
            "457\n",
            "2328\n",
            "2249\n",
            "3410\n",
            "1692\n",
            "2027\n",
            "3289\n",
            "818\n",
            "1153\n",
            "3719\n",
            "2639\n",
            "5151\n",
            "3044\n",
            "3100\n",
            "2153\n",
            "120\n",
            "663\n",
            "2287\n",
            "2918\n",
            "1116\n",
            "238\n",
            "872\n",
            "2604\n",
            "3021\n",
            "821\n",
            "1690\n",
            "2962\n",
            "2928\n",
            "2943\n",
            "585\n",
            "2645\n",
            "11231\n",
            "2087\n",
            "2478\n",
            "892\n",
            "1733\n",
            "2797\n",
            "2451\n",
            "118\n",
            "2542\n",
            "575\n",
            "2605\n",
            "2596\n",
            "483\n",
            "604\n",
            "488\n",
            "148\n",
            "386\n",
            "712\n",
            "2088\n",
            "98\n",
            "137\n",
            "3207\n",
            "1786\n",
            "1012\n",
            "2496\n",
            "524\n",
            "2103\n",
            "1593\n",
            "1390\n",
            "2339\n",
            "4625\n",
            "2965\n",
            "1651\n",
            "2600\n",
            "1034\n",
            "2734\n",
            "2495\n",
            "3708\n",
            "3450\n",
            "773\n",
            "1074\n",
            "4918\n",
            "685\n",
            "115\n",
            "1248\n",
            "1716\n",
            "3030\n",
            "626\n",
            "728\n",
            "2418\n",
            "1754\n",
            "162\n",
            "785\n",
            "2072\n",
            "3518\n",
            "3128\n",
            "469\n",
            "1532\n",
            "2206\n",
            "3380\n",
            "2094\n",
            "2882\n",
            "1506\n",
            "2013\n",
            "138\n",
            "1008\n",
            "1324\n",
            "2776\n",
            "3521\n",
            "959\n",
            "5571\n",
            "4981\n",
            "2055\n",
            "2580\n",
            "138\n",
            "5185\n",
            "2187\n",
            "1486\n",
            "613\n",
            "1915\n",
            "1411\n",
            "3003\n",
            "3374\n",
            "3034\n",
            "3595\n",
            "2593\n",
            "1998\n",
            "1117\n",
            "2503\n",
            "164\n",
            "2624\n",
            "2977\n",
            "1731\n",
            "1753\n",
            "2179\n",
            "6751\n",
            "135\n",
            "5390\n",
            "3493\n",
            "2728\n",
            "1045\n",
            "223\n",
            "596\n",
            "1364\n",
            "4735\n",
            "1590\n",
            "723\n",
            "0\n",
            "2281\n",
            "3322\n",
            "7737\n",
            "164\n",
            "2609\n",
            "1404\n",
            "3204\n",
            "1402\n",
            "2904\n",
            "132\n",
            "219\n",
            "877\n",
            "3250\n",
            "2061\n",
            "2751\n",
            "3474\n",
            "2801\n",
            "3768\n",
            "3025\n",
            "1545\n",
            "2830\n",
            "4222\n",
            "124\n",
            "10144\n",
            "2884\n",
            "837\n",
            "3127\n",
            "2825\n",
            "243\n",
            "978\n",
            "2789\n",
            "99\n",
            "2956\n",
            "1001\n",
            "1911\n",
            "3548\n",
            "710\n",
            "3809\n",
            "1849\n",
            "1017\n",
            "2340\n",
            "1849\n",
            "1240\n",
            "1759\n",
            "1024\n",
            "1919\n",
            "6689\n",
            "567\n",
            "3123\n",
            "4177\n",
            "2439\n",
            "3206\n",
            "2922\n",
            "1169\n",
            "1470\n",
            "409\n",
            "187\n",
            "3597\n",
            "1709\n",
            "1134\n",
            "1264\n",
            "1991\n",
            "1513\n",
            "377\n",
            "136\n",
            "3155\n",
            "3770\n",
            "3313\n",
            "161\n",
            "195\n",
            "967\n",
            "886\n",
            "2225\n",
            "561\n",
            "841\n",
            "108\n",
            "766\n",
            "1554\n",
            "827\n",
            "2067\n",
            "3739\n",
            "427\n",
            "1688\n",
            "577\n",
            "1321\n",
            "805\n",
            "180\n",
            "940\n",
            "354\n",
            "123\n",
            "119\n",
            "5288\n",
            "1863\n",
            "195\n",
            "4141\n",
            "1794\n",
            "205\n",
            "220\n",
            "1754\n",
            "3036\n",
            "4945\n",
            "114\n",
            "4138\n",
            "3628\n",
            "2139\n",
            "1392\n",
            "307\n",
            "3930\n",
            "3964\n",
            "2719\n",
            "2186\n",
            "126\n",
            "2886\n",
            "1878\n",
            "1030\n",
            "2242\n",
            "146\n",
            "949\n",
            "2471\n",
            "2082\n",
            "4785\n",
            "398\n",
            "574\n",
            "3122\n",
            "531\n",
            "2744\n",
            "4249\n",
            "183\n",
            "1680\n",
            "2477\n",
            "1174\n",
            "2547\n",
            "472\n",
            "2922\n",
            "2976\n",
            "1029\n",
            "1847\n",
            "517\n",
            "107\n",
            "328\n",
            "638\n",
            "4213\n",
            "373\n",
            "3251\n",
            "748\n",
            "1907\n",
            "2449\n",
            "1441\n",
            "1711\n",
            "2512\n",
            "3384\n",
            "3561\n",
            "1425\n",
            "3241\n",
            "866\n",
            "4088\n",
            "2102\n",
            "2698\n",
            "642\n",
            "2681\n",
            "633\n",
            "3349\n",
            "3477\n",
            "2305\n",
            "1957\n",
            "644\n",
            "648\n",
            "2311\n",
            "3235\n",
            "2833\n",
            "224\n",
            "3708\n",
            "98\n",
            "2518\n",
            "1163\n",
            "512\n",
            "864\n",
            "923\n",
            "234\n",
            "3370\n",
            "1520\n",
            "1362\n",
            "2280\n",
            "5601\n",
            "342\n",
            "856\n",
            "1244\n",
            "2061\n",
            "759\n",
            "3300\n",
            "280\n",
            "2093\n",
            "157\n",
            "2229\n",
            "3097\n",
            "1637\n",
            "538\n",
            "2521\n",
            "3726\n",
            "125\n",
            "537\n",
            "405\n",
            "730\n",
            "2964\n",
            "140\n",
            "534\n",
            "157\n",
            "129\n",
            "1032\n",
            "212\n",
            "325\n",
            "162\n",
            "2289\n",
            "803\n",
            "4293\n",
            "261\n",
            "3743\n",
            "1892\n",
            "1568\n",
            "2308\n",
            "2447\n",
            "605\n",
            "6481\n",
            "1063\n",
            "2325\n",
            "3010\n",
            "496\n",
            "2431\n",
            "507\n",
            "161\n",
            "679\n",
            "570\n",
            "260\n",
            "2658\n",
            "1555\n",
            "3098\n",
            "1287\n",
            "2205\n",
            "3491\n",
            "127\n",
            "1050\n",
            "70\n",
            "737\n",
            "3323\n",
            "1098\n",
            "1758\n",
            "2088\n",
            "209\n",
            "183\n",
            "1508\n",
            "261\n",
            "1304\n",
            "1638\n",
            "247\n",
            "266\n",
            "2243\n",
            "4354\n",
            "1801\n",
            "1500\n",
            "2517\n",
            "2553\n",
            "1843\n",
            "542\n",
            "2485\n",
            "2494\n",
            "819\n",
            "5652\n",
            "5985\n",
            "2197\n",
            "2523\n",
            "849\n",
            "248\n",
            "1530\n",
            "1201\n",
            "977\n",
            "4142\n",
            "2751\n",
            "1425\n",
            "1105\n",
            "0\n",
            "2698\n",
            "2498\n",
            "194\n",
            "1609\n",
            "3421\n",
            "398\n",
            "1314\n",
            "1811\n",
            "236\n",
            "1895\n",
            "645\n",
            "1512\n",
            "1340\n",
            "2739\n",
            "1523\n",
            "1582\n",
            "1095\n",
            "158\n",
            "370\n",
            "3170\n",
            "1808\n",
            "191\n",
            "12018\n",
            "77\n",
            "1328\n",
            "1150\n",
            "2440\n",
            "2848\n",
            "4549\n",
            "4039\n",
            "3042\n",
            "2687\n",
            "1006\n",
            "3504\n",
            "3735\n",
            "149\n",
            "1250\n",
            "1381\n",
            "1826\n",
            "3793\n",
            "3493\n",
            "1100\n",
            "3326\n",
            "196\n",
            "4550\n",
            "2290\n",
            "1412\n",
            "851\n",
            "2461\n",
            "583\n",
            "726\n",
            "2939\n",
            "298\n",
            "1658\n",
            "2010\n",
            "1953\n",
            "281\n",
            "4198\n",
            "3640\n",
            "2611\n",
            "3473\n",
            "1377\n",
            "786\n",
            "310\n",
            "1080\n",
            "1682\n",
            "166\n",
            "2025\n",
            "5964\n",
            "1310\n",
            "2228\n",
            "2586\n",
            "1913\n",
            "4649\n",
            "6255\n",
            "2882\n",
            "3354\n",
            "1567\n",
            "2162\n",
            "1023\n",
            "922\n",
            "663\n",
            "382\n",
            "2176\n",
            "4828\n",
            "4899\n",
            "324\n",
            "1978\n",
            "1307\n",
            "516\n",
            "166\n",
            "838\n",
            "165\n",
            "2425\n",
            "2855\n",
            "2652\n",
            "2643\n",
            "2354\n",
            "3130\n",
            "130\n",
            "684\n",
            "3708\n",
            "1848\n",
            "442\n",
            "3187\n",
            "1634\n",
            "2271\n",
            "307\n",
            "1750\n",
            "2264\n",
            "263\n",
            "3065\n",
            "6542\n",
            "2107\n",
            "2543\n",
            "2887\n",
            "3921\n",
            "1070\n",
            "2358\n",
            "2949\n",
            "166\n",
            "3285\n",
            "1703\n",
            "219\n",
            "748\n",
            "69\n",
            "2616\n",
            "2693\n",
            "2472\n",
            "274\n",
            "672\n",
            "710\n",
            "2372\n",
            "1021\n",
            "3295\n",
            "1997\n",
            "146\n",
            "2021\n",
            "1133\n",
            "1451\n",
            "211\n",
            "1100\n",
            "1287\n",
            "0\n",
            "756\n",
            "1317\n",
            "208\n",
            "1634\n",
            "3667\n",
            "4393\n",
            "1052\n",
            "319\n",
            "904\n",
            "303\n",
            "1555\n",
            "139\n",
            "3091\n",
            "1584\n",
            "3379\n",
            "811\n",
            "1746\n",
            "1065\n",
            "403\n",
            "2982\n",
            "1171\n",
            "2251\n",
            "2622\n",
            "1907\n",
            "152\n",
            "2860\n",
            "281\n",
            "2269\n",
            "2617\n",
            "2338\n",
            "210\n",
            "2260\n",
            "2691\n",
            "678\n",
            "168\n",
            "219\n",
            "1255\n",
            "0\n",
            "1416\n",
            "2343\n",
            "6104\n",
            "1147\n",
            "2503\n",
            "1014\n",
            "1715\n",
            "3056\n",
            "1797\n",
            "14881\n",
            "3195\n",
            "1778\n",
            "4645\n",
            "498\n",
            "1143\n",
            "1567\n",
            "1994\n",
            "95\n",
            "140\n",
            "2417\n",
            "914\n",
            "2431\n",
            "2180\n",
            "436\n",
            "3413\n",
            "806\n",
            "3330\n",
            "5455\n",
            "3612\n",
            "1610\n",
            "1305\n",
            "1855\n",
            "2380\n",
            "3615\n",
            "1839\n",
            "2423\n",
            "2624\n",
            "2351\n",
            "3856\n",
            "2868\n",
            "179\n",
            "964\n",
            "477\n",
            "1310\n",
            "3071\n",
            "2094\n",
            "1149\n",
            "3653\n",
            "13108\n",
            "4670\n",
            "653\n",
            "3096\n",
            "1190\n",
            "523\n",
            "153\n",
            "2953\n",
            "1167\n",
            "1276\n",
            "802\n",
            "3395\n",
            "626\n",
            "4036\n",
            "620\n",
            "2195\n",
            "1996\n",
            "2078\n",
            "1674\n",
            "2995\n",
            "11296\n",
            "579\n",
            "444\n",
            "1206\n",
            "103\n",
            "2400\n",
            "127\n",
            "1268\n",
            "2799\n",
            "851\n",
            "1353\n",
            "1682\n",
            "1278\n",
            "3273\n",
            "2400\n",
            "786\n",
            "1171\n",
            "1933\n",
            "3242\n",
            "978\n",
            "1449\n",
            "3174\n",
            "176\n",
            "255\n",
            "715\n",
            "1409\n",
            "1443\n",
            "1468\n",
            "5862\n",
            "4391\n",
            "2352\n",
            "1103\n",
            "3669\n",
            "2683\n",
            "2180\n",
            "1006\n",
            "1247\n",
            "2679\n",
            "129\n",
            "2574\n",
            "573\n",
            "1591\n",
            "726\n",
            "126\n",
            "3737\n",
            "5222\n",
            "2567\n",
            "2451\n",
            "672\n",
            "2968\n",
            "137\n",
            "3791\n",
            "4221\n",
            "217\n",
            "1256\n",
            "727\n",
            "166\n",
            "1871\n",
            "3937\n",
            "111\n",
            "13619\n",
            "1398\n",
            "4481\n",
            "751\n",
            "543\n",
            "715\n",
            "1341\n",
            "2320\n",
            "2330\n",
            "2859\n",
            "180\n",
            "1512\n",
            "925\n",
            "2862\n",
            "1805\n",
            "818\n",
            "2860\n",
            "2637\n",
            "2927\n",
            "3073\n",
            "603\n",
            "3655\n",
            "5119\n",
            "3197\n",
            "1217\n",
            "518\n",
            "246\n",
            "325\n",
            "5252\n",
            "209\n",
            "3086\n",
            "2941\n",
            "2044\n",
            "254\n",
            "289\n",
            "1124\n",
            "3143\n",
            "491\n",
            "2185\n",
            "1504\n",
            "8114\n",
            "281\n",
            "235\n",
            "2679\n",
            "1249\n",
            "221\n",
            "1789\n",
            "107\n",
            "530\n",
            "4231\n",
            "2856\n",
            "2109\n",
            "2149\n",
            "2218\n",
            "2143\n",
            "2529\n",
            "379\n",
            "871\n",
            "104\n",
            "223\n",
            "1759\n",
            "2178\n",
            "842\n",
            "2492\n",
            "901\n",
            "721\n",
            "1470\n",
            "1122\n",
            "4218\n",
            "968\n",
            "900\n",
            "2939\n",
            "1707\n",
            "1881\n",
            "4891\n",
            "3371\n",
            "1459\n",
            "923\n",
            "2121\n",
            "1587\n",
            "3128\n",
            "2592\n",
            "1052\n",
            "2443\n",
            "7486\n",
            "2518\n",
            "1681\n",
            "4528\n",
            "447\n",
            "766\n",
            "1435\n",
            "2134\n",
            "2027\n",
            "867\n",
            "4874\n",
            "3203\n",
            "0\n",
            "675\n",
            "901\n",
            "2714\n",
            "4151\n",
            "2857\n",
            "3733\n",
            "408\n",
            "1172\n",
            "4496\n",
            "2319\n",
            "1148\n",
            "5343\n",
            "1209\n",
            "4335\n",
            "1291\n",
            "1075\n",
            "703\n",
            "1433\n",
            "1413\n",
            "298\n",
            "454\n",
            "1112\n",
            "2035\n",
            "686\n",
            "5476\n",
            "726\n",
            "1150\n",
            "3434\n",
            "4051\n",
            "2330\n",
            "554\n",
            "2921\n",
            "149\n",
            "2600\n",
            "2874\n",
            "1336\n",
            "4624\n",
            "933\n",
            "699\n",
            "8497\n",
            "370\n",
            "172\n",
            "408\n",
            "621\n",
            "5116\n",
            "1499\n",
            "1081\n",
            "3163\n",
            "175\n",
            "2651\n",
            "1084\n",
            "1509\n",
            "480\n",
            "2614\n",
            "7050\n",
            "2731\n",
            "2835\n",
            "998\n",
            "3910\n",
            "3873\n",
            "391\n",
            "2444\n",
            "311\n",
            "248\n",
            "1339\n",
            "845\n",
            "2123\n",
            "1006\n",
            "912\n",
            "3420\n",
            "4726\n",
            "578\n",
            "1407\n",
            "1721\n",
            "3288\n",
            "828\n",
            "900\n",
            "3562\n",
            "2266\n",
            "1333\n",
            "4624\n",
            "1743\n",
            "1701\n",
            "1024\n",
            "1529\n",
            "1810\n",
            "1671\n",
            "1871\n",
            "668\n",
            "784\n",
            "2330\n",
            "3262\n",
            "4423\n",
            "2110\n",
            "115\n",
            "2498\n",
            "2069\n",
            "356\n",
            "2806\n",
            "397\n",
            "1200\n",
            "2909\n",
            "2798\n",
            "2665\n",
            "3183\n",
            "3064\n",
            "463\n",
            "175\n",
            "1514\n",
            "1061\n",
            "415\n",
            "3293\n",
            "3281\n",
            "1119\n",
            "167\n",
            "973\n",
            "555\n",
            "100\n",
            "3662\n",
            "1151\n",
            "103\n",
            "1815\n",
            "1657\n",
            "2929\n",
            "4194\n",
            "2869\n",
            "447\n",
            "3922\n",
            "2475\n",
            "3003\n",
            "3726\n",
            "166\n",
            "1664\n",
            "1203\n",
            "2708\n",
            "10396\n",
            "764\n",
            "1711\n",
            "456\n",
            "2795\n",
            "85\n",
            "963\n",
            "351\n",
            "1287\n",
            "1994\n",
            "2636\n",
            "4250\n",
            "2341\n",
            "649\n",
            "3207\n",
            "530\n",
            "1911\n",
            "2553\n",
            "659\n",
            "895\n",
            "1177\n",
            "3221\n",
            "2927\n",
            "2469\n",
            "2912\n",
            "978\n",
            "1044\n",
            "2641\n",
            "1732\n",
            "2975\n",
            "1093\n",
            "3893\n",
            "153\n",
            "1914\n",
            "1501\n",
            "5654\n",
            "3215\n",
            "951\n",
            "3583\n",
            "1409\n",
            "856\n",
            "2780\n",
            "638\n",
            "2872\n",
            "295\n",
            "3726\n",
            "77\n",
            "3290\n",
            "4057\n",
            "0\n",
            "2575\n",
            "157\n",
            "3883\n",
            "1072\n",
            "1330\n",
            "1385\n",
            "960\n",
            "1745\n",
            "2660\n",
            "5893\n",
            "1123\n",
            "2617\n",
            "1160\n",
            "531\n",
            "1659\n",
            "853\n",
            "969\n",
            "146\n",
            "1758\n",
            "2821\n",
            "809\n",
            "2763\n",
            "351\n",
            "6310\n",
            "2193\n",
            "3504\n",
            "4051\n",
            "2785\n",
            "1778\n",
            "3402\n",
            "190\n",
            "2023\n",
            "202\n",
            "3554\n",
            "3766\n",
            "963\n",
            "1404\n",
            "72\n",
            "3811\n",
            "11759\n",
            "516\n",
            "1085\n",
            "2297\n",
            "2161\n",
            "744\n",
            "941\n",
            "159\n",
            "942\n",
            "1747\n",
            "1165\n",
            "2961\n",
            "1099\n",
            "162\n",
            "2021\n",
            "2892\n",
            "821\n",
            "1388\n",
            "644\n",
            "5773\n",
            "3592\n",
            "512\n",
            "863\n",
            "3478\n",
            "436\n",
            "1464\n",
            "2935\n",
            "393\n",
            "3091\n",
            "1146\n",
            "1093\n",
            "2185\n",
            "1284\n",
            "2124\n",
            "10885\n",
            "122\n",
            "2447\n",
            "900\n",
            "977\n",
            "608\n",
            "0\n",
            "2772\n",
            "1437\n",
            "2288\n",
            "3609\n",
            "1105\n",
            "2800\n",
            "569\n",
            "556\n",
            "207\n",
            "640\n",
            "7613\n",
            "1364\n",
            "2475\n",
            "350\n",
            "1783\n",
            "1682\n",
            "1474\n",
            "2987\n",
            "728\n",
            "1541\n",
            "1020\n",
            "186\n",
            "674\n",
            "142\n",
            "1594\n",
            "898\n",
            "3200\n",
            "129\n",
            "2330\n",
            "3030\n",
            "1035\n",
            "2119\n",
            "4452\n",
            "1463\n",
            "156\n",
            "2598\n",
            "3705\n",
            "941\n",
            "102\n",
            "1749\n",
            "1122\n",
            "4580\n",
            "223\n",
            "555\n",
            "1343\n",
            "430\n",
            "4157\n",
            "2782\n",
            "3486\n",
            "1825\n",
            "82\n",
            "1633\n",
            "314\n",
            "3689\n",
            "71\n",
            "2091\n",
            "4461\n",
            "407\n",
            "665\n",
            "921\n",
            "126\n",
            "0\n",
            "3121\n",
            "1959\n",
            "1700\n",
            "147\n",
            "599\n",
            "517\n",
            "1885\n",
            "1453\n",
            "2476\n",
            "2021\n",
            "235\n",
            "1460\n",
            "3968\n",
            "1980\n",
            "327\n",
            "2074\n",
            "683\n",
            "199\n",
            "2567\n",
            "752\n",
            "644\n",
            "2665\n",
            "2053\n",
            "294\n",
            "1629\n",
            "129\n",
            "1763\n",
            "542\n",
            "3207\n",
            "4523\n",
            "276\n",
            "2732\n",
            "2362\n",
            "2101\n",
            "2208\n",
            "4483\n",
            "1325\n",
            "1800\n",
            "134\n",
            "1749\n",
            "2119\n",
            "3489\n",
            "2285\n",
            "2160\n",
            "1363\n",
            "811\n",
            "1434\n",
            "1562\n",
            "956\n",
            "2641\n",
            "434\n",
            "9435\n",
            "608\n",
            "382\n",
            "2374\n",
            "1785\n",
            "1368\n",
            "6365\n",
            "284\n",
            "0\n",
            "900\n",
            "548\n",
            "2539\n",
            "584\n",
            "3009\n",
            "4140\n",
            "3322\n",
            "2401\n",
            "4760\n",
            "2336\n",
            "3296\n",
            "1436\n",
            "1106\n",
            "2024\n",
            "2016\n",
            "2083\n",
            "803\n",
            "3744\n",
            "0\n",
            "1712\n",
            "911\n",
            "614\n",
            "1348\n",
            "495\n",
            "4907\n",
            "316\n",
            "232\n",
            "355\n",
            "2248\n",
            "1683\n",
            "3301\n",
            "124\n",
            "1096\n",
            "346\n",
            "4436\n",
            "2787\n",
            "384\n",
            "1009\n",
            "980\n",
            "1625\n",
            "2294\n",
            "7137\n",
            "2359\n",
            "1090\n",
            "517\n",
            "261\n",
            "5408\n",
            "122\n",
            "3105\n",
            "2359\n",
            "2484\n",
            "691\n",
            "283\n",
            "1783\n",
            "1603\n",
            "945\n",
            "2585\n",
            "1299\n",
            "665\n",
            "2853\n",
            "1249\n",
            "145\n",
            "2666\n",
            "1489\n",
            "1946\n",
            "140\n",
            "probably a utf-8 error\n",
            "4194\n",
            "1542\n",
            "1206\n",
            "530\n",
            "3104\n",
            "3954\n",
            "3122\n",
            "1877\n",
            "3398\n",
            "124\n",
            "1778\n",
            "1576\n",
            "842\n",
            "1450\n",
            "1639\n",
            "1101\n",
            "3540\n",
            "116\n",
            "2778\n",
            "1818\n",
            "5371\n",
            "3655\n",
            "1279\n",
            "3287\n",
            "279\n",
            "2872\n",
            "1733\n",
            "122\n",
            "1035\n",
            "106\n",
            "5021\n",
            "1708\n",
            "593\n",
            "2505\n",
            "3375\n",
            "733\n",
            "2332\n",
            "7253\n",
            "3491\n",
            "2806\n",
            "106\n",
            "5231\n",
            "2415\n",
            "4132\n",
            "1485\n",
            "0\n",
            "1951\n",
            "2787\n",
            "2360\n",
            "344\n",
            "3450\n",
            "2861\n",
            "657\n",
            "4403\n",
            "2461\n",
            "206\n",
            "180\n",
            "2328\n",
            "255\n",
            "1456\n",
            "137\n",
            "1815\n",
            "2281\n",
            "2166\n",
            "1866\n",
            "680\n",
            "1000\n",
            "6055\n",
            "3015\n",
            "2699\n",
            "1057\n",
            "1734\n",
            "1974\n",
            "828\n",
            "2810\n",
            "2447\n",
            "373\n",
            "6142\n",
            "2179\n",
            "2373\n",
            "3556\n",
            "1806\n",
            "2445\n",
            "2445\n",
            "2515\n",
            "3051\n",
            "3821\n",
            "2918\n",
            "3165\n",
            "4022\n",
            "3558\n",
            "685\n",
            "593\n",
            "2208\n",
            "763\n",
            "2399\n",
            "635\n",
            "928\n",
            "541\n",
            "620\n",
            "1707\n",
            "3339\n",
            "2282\n",
            "2178\n",
            "3081\n",
            "3004\n",
            "293\n",
            "906\n",
            "613\n",
            "1456\n",
            "2167\n",
            "431\n",
            "1028\n",
            "577\n",
            "3869\n",
            "638\n",
            "1925\n",
            "359\n",
            "517\n",
            "3387\n",
            "4363\n",
            "3146\n",
            "1428\n",
            "2137\n",
            "1817\n",
            "4580\n",
            "1725\n",
            "911\n",
            "3276\n",
            "1594\n",
            "5628\n",
            "1672\n",
            "720\n",
            "544\n",
            "1766\n",
            "73\n",
            "150\n",
            "1021\n",
            "230\n",
            "1160\n",
            "4399\n",
            "3301\n",
            "1723\n",
            "2333\n",
            "2383\n",
            "75\n",
            "493\n",
            "346\n",
            "254\n",
            "955\n",
            "976\n",
            "3722\n",
            "5324\n",
            "198\n",
            "1539\n",
            "223\n",
            "2900\n",
            "3329\n",
            "3404\n",
            "4021\n",
            "2248\n",
            "2828\n",
            "2846\n",
            "1438\n",
            "3382\n",
            "1662\n",
            "2791\n",
            "3414\n",
            "2745\n",
            "123\n",
            "2614\n",
            "1058\n",
            "1335\n",
            "279\n",
            "1470\n",
            "2114\n",
            "2756\n",
            "3270\n",
            "3322\n",
            "1653\n",
            "1162\n",
            "570\n",
            "362\n",
            "654\n",
            "0\n",
            "5259\n",
            "2923\n",
            "1459\n",
            "2151\n",
            "1186\n",
            "1458\n",
            "1090\n",
            "1096\n",
            "5605\n",
            "72\n",
            "3421\n",
            "1016\n",
            "1426\n",
            "1645\n",
            "3561\n",
            "312\n",
            "2257\n",
            "3297\n",
            "127\n",
            "223\n",
            "2379\n",
            "845\n",
            "966\n",
            "3675\n",
            "147\n",
            "2192\n",
            "1442\n",
            "1268\n",
            "2594\n",
            "2957\n",
            "2272\n",
            "11119\n",
            "2492\n",
            "1687\n",
            "128\n",
            "158\n",
            "897\n",
            "3021\n",
            "2411\n",
            "1461\n",
            "9099\n",
            "2009\n",
            "189\n",
            "231\n",
            "2230\n",
            "3810\n",
            "1990\n",
            "73\n",
            "739\n",
            "3098\n",
            "2922\n",
            "157\n",
            "681\n",
            "156\n",
            "1238\n",
            "2453\n",
            "2756\n",
            "693\n",
            "72\n",
            "283\n",
            "2352\n",
            "4207\n",
            "1078\n",
            "2157\n",
            "3015\n",
            "226\n",
            "2544\n",
            "3865\n",
            "4104\n",
            "2341\n",
            "2997\n",
            "1523\n",
            "249\n",
            "875\n",
            "3116\n",
            "10731\n",
            "834\n",
            "2073\n",
            "869\n",
            "2080\n",
            "238\n",
            "2809\n",
            "2840\n",
            "2316\n",
            "5460\n",
            "1789\n",
            "3140\n",
            "560\n",
            "1751\n",
            "76\n",
            "997\n",
            "2513\n",
            "3022\n",
            "127\n",
            "629\n",
            "2272\n",
            "2129\n",
            "2011\n",
            "689\n",
            "2434\n",
            "1877\n",
            "1586\n",
            "5550\n",
            "148\n",
            "2597\n",
            "650\n",
            "984\n",
            "2078\n",
            "5198\n",
            "4560\n",
            "3353\n",
            "1512\n",
            "117\n",
            "1054\n",
            "1109\n",
            "434\n",
            "1398\n",
            "437\n",
            "2402\n",
            "2701\n",
            "352\n",
            "3455\n",
            "1093\n",
            "4633\n",
            "3602\n",
            "1566\n",
            "2420\n",
            "728\n",
            "306\n",
            "3128\n",
            "1760\n",
            "102\n",
            "157\n",
            "4446\n",
            "837\n",
            "1795\n",
            "2834\n",
            "1568\n",
            "1743\n",
            "1454\n",
            "1546\n",
            "2532\n",
            "1127\n",
            "212\n",
            "4456\n",
            "580\n",
            "677\n",
            "1350\n",
            "790\n",
            "2515\n",
            "3065\n",
            "2243\n",
            "3053\n",
            "3212\n",
            "1437\n",
            "116\n",
            "5190\n",
            "4380\n",
            "1483\n",
            "2269\n",
            "3041\n",
            "161\n",
            "2713\n",
            "316\n",
            "336\n",
            "896\n",
            "1964\n",
            "1339\n",
            "155\n",
            "3015\n",
            "1780\n",
            "3479\n",
            "1560\n",
            "3269\n",
            "1299\n",
            "1214\n",
            "4874\n",
            "1204\n",
            "1392\n",
            "1228\n",
            "805\n",
            "2134\n",
            "1748\n",
            "3138\n",
            "2003\n",
            "3554\n",
            "1915\n",
            "459\n",
            "573\n",
            "1391\n",
            "917\n",
            "1112\n",
            "964\n",
            "3595\n",
            "0\n",
            "177\n",
            "437\n",
            "660\n",
            "3050\n",
            "3434\n",
            "951\n",
            "4534\n",
            "2703\n",
            "3198\n",
            "3935\n",
            "3427\n",
            "188\n",
            "5526\n",
            "150\n",
            "125\n",
            "3361\n",
            "170\n",
            "2473\n",
            "1115\n",
            "1865\n",
            "1623\n",
            "1894\n",
            "888\n",
            "3947\n",
            "162\n",
            "571\n",
            "3218\n",
            "810\n",
            "1035\n",
            "1385\n",
            "1214\n",
            "326\n",
            "5737\n",
            "0\n",
            "839\n",
            "144\n",
            "2862\n",
            "2817\n",
            "1873\n",
            "3984\n",
            "852\n",
            "1862\n",
            "3313\n",
            "2500\n",
            "1501\n",
            "1180\n",
            "1617\n",
            "1162\n",
            "760\n",
            "1673\n",
            "1678\n",
            "1591\n",
            "1054\n",
            "2994\n",
            "1842\n",
            "3243\n",
            "1699\n",
            "704\n",
            "1186\n",
            "1139\n",
            "2839\n",
            "1427\n",
            "1289\n",
            "1328\n",
            "1996\n",
            "1933\n",
            "3205\n",
            "2869\n",
            "3290\n",
            "4386\n",
            "2244\n",
            "2537\n",
            "2390\n",
            "4411\n",
            "1295\n",
            "3370\n",
            "2443\n",
            "2178\n",
            "856\n",
            "501\n",
            "1275\n",
            "623\n",
            "3424\n",
            "626\n",
            "1293\n",
            "1697\n",
            "785\n",
            "2409\n",
            "0\n",
            "2655\n",
            "138\n",
            "3269\n",
            "341\n",
            "2051\n",
            "1629\n",
            "167\n",
            "3443\n",
            "721\n",
            "594\n",
            "2267\n",
            "852\n",
            "826\n",
            "1888\n",
            "338\n",
            "878\n",
            "2295\n",
            "1560\n",
            "142\n",
            "1873\n",
            "1622\n",
            "3151\n",
            "901\n",
            "2346\n",
            "883\n",
            "1016\n",
            "0\n",
            "2649\n",
            "1106\n",
            "1202\n",
            "816\n",
            "687\n",
            "133\n",
            "9073\n",
            "994\n",
            "573\n",
            "570\n",
            "2213\n",
            "114\n",
            "0\n",
            "3272\n",
            "691\n",
            "181\n",
            "606\n",
            "1391\n",
            "1970\n",
            "1091\n",
            "2112\n",
            "0\n",
            "4310\n",
            "1477\n",
            "2413\n",
            "4744\n",
            "1780\n",
            "706\n",
            "778\n",
            "498\n",
            "1886\n",
            "2075\n",
            "2348\n",
            "631\n",
            "1509\n",
            "101\n",
            "1872\n",
            "1002\n",
            "3745\n",
            "2735\n",
            "3260\n",
            "207\n",
            "1732\n",
            "1610\n",
            "2777\n",
            "2092\n",
            "2419\n",
            "228\n",
            "4136\n",
            "1720\n",
            "160\n",
            "537\n",
            "2913\n",
            "145\n",
            "2651\n",
            "760\n",
            "2350\n",
            "2007\n",
            "3368\n",
            "4316\n",
            "2179\n",
            "2054\n",
            "576\n",
            "1483\n",
            "1857\n",
            "2645\n",
            "2048\n",
            "1208\n",
            "250\n",
            "1470\n",
            "0\n",
            "1271\n",
            "3375\n",
            "527\n",
            "124\n",
            "1593\n",
            "1528\n",
            "161\n",
            "960\n",
            "746\n",
            "160\n",
            "1910\n",
            "3584\n",
            "3041\n",
            "341\n",
            "846\n",
            "1119\n",
            "1965\n",
            "2772\n",
            "2058\n",
            "619\n",
            "1850\n",
            "4539\n",
            "73\n",
            "7280\n",
            "384\n",
            "0\n",
            "2695\n",
            "632\n",
            "977\n",
            "154\n",
            "239\n",
            "1816\n",
            "129\n",
            "348\n",
            "990\n",
            "1906\n",
            "2939\n",
            "4773\n",
            "3653\n",
            "1521\n",
            "1230\n",
            "6365\n",
            "165\n",
            "7920\n",
            "2199\n",
            "782\n",
            "2181\n",
            "717\n",
            "2877\n",
            "811\n",
            "3523\n",
            "2437\n",
            "216\n",
            "952\n",
            "2932\n",
            "1262\n",
            "1144\n",
            "149\n",
            "3601\n",
            "897\n",
            "73\n",
            "2441\n",
            "3021\n",
            "2456\n",
            "1011\n",
            "236\n",
            "1176\n",
            "2961\n",
            "1882\n",
            "231\n",
            "2324\n",
            "1219\n",
            "485\n",
            "934\n",
            "348\n",
            "1045\n",
            "1814\n",
            "1004\n",
            "1252\n",
            "114\n",
            "3807\n",
            "2296\n",
            "3861\n",
            "383\n",
            "210\n",
            "71\n",
            "180\n",
            "215\n",
            "2572\n",
            "2012\n",
            "964\n",
            "176\n",
            "336\n",
            "2444\n",
            "2269\n",
            "4899\n",
            "3314\n",
            "1059\n",
            "111\n",
            "952\n",
            "13602\n",
            "889\n",
            "2710\n",
            "703\n",
            "152\n",
            "3370\n",
            "1536\n",
            "1133\n",
            "3272\n",
            "1904\n",
            "794\n",
            "3796\n",
            "3708\n",
            "0\n",
            "5955\n",
            "2604\n",
            "1323\n",
            "6894\n",
            "592\n",
            "2884\n",
            "135\n",
            "2482\n",
            "564\n",
            "2612\n",
            "188\n",
            "926\n",
            "1275\n",
            "671\n",
            "1335\n",
            "3954\n",
            "3133\n",
            "225\n",
            "1889\n",
            "72\n",
            "3606\n",
            "2745\n",
            "1516\n",
            "3157\n",
            "4445\n",
            "1247\n",
            "2359\n",
            "386\n",
            "2932\n",
            "711\n",
            "3038\n",
            "2552\n",
            "1694\n",
            "2141\n",
            "175\n",
            "2138\n",
            "186\n",
            "1257\n",
            "2697\n",
            "3275\n",
            "3832\n",
            "2217\n",
            "1747\n",
            "4584\n",
            "872\n",
            "169\n",
            "2390\n",
            "637\n",
            "3448\n",
            "2780\n",
            "591\n",
            "202\n",
            "257\n",
            "675\n",
            "3138\n",
            "1184\n",
            "1147\n",
            "5849\n",
            "3562\n",
            "2061\n",
            "301\n",
            "374\n",
            "497\n",
            "2468\n",
            "3054\n",
            "640\n",
            "2790\n",
            "677\n",
            "726\n",
            "2335\n",
            "1491\n",
            "4675\n",
            "717\n",
            "2144\n",
            "2847\n",
            "570\n",
            "2698\n",
            "705\n",
            "4310\n",
            "356\n",
            "1562\n",
            "124\n",
            "1997\n",
            "2563\n",
            "3134\n",
            "1484\n",
            "2018\n",
            "1068\n",
            "1191\n",
            "1014\n",
            "1386\n",
            "3227\n",
            "641\n",
            "2872\n",
            "1429\n",
            "73\n",
            "3206\n",
            "3808\n",
            "168\n",
            "1392\n",
            "1532\n",
            "3476\n",
            "11573\n",
            "4650\n",
            "124\n",
            "2142\n",
            "2931\n",
            "2494\n",
            "3226\n",
            "546\n",
            "5157\n",
            "541\n",
            "2224\n",
            "2364\n",
            "3675\n",
            "166\n",
            "2743\n",
            "3718\n",
            "1860\n",
            "264\n",
            "270\n",
            "137\n",
            "622\n",
            "982\n",
            "3810\n",
            "212\n",
            "2395\n",
            "3876\n",
            "797\n",
            "1168\n",
            "2559\n",
            "1896\n",
            "1049\n",
            "957\n",
            "740\n",
            "812\n",
            "782\n",
            "493\n",
            "138\n",
            "185\n",
            "2928\n",
            "949\n",
            "3218\n",
            "904\n",
            "2250\n",
            "267\n",
            "1049\n",
            "2648\n",
            "1687\n",
            "1304\n",
            "2098\n",
            "153\n",
            "1904\n",
            "373\n",
            "1676\n",
            "999\n",
            "678\n",
            "7675\n",
            "1037\n",
            "2569\n",
            "1258\n",
            "1339\n",
            "288\n",
            "168\n",
            "2650\n",
            "1381\n",
            "3480\n",
            "915\n",
            "777\n",
            "1400\n",
            "1522\n",
            "2766\n",
            "2343\n",
            "211\n",
            "798\n",
            "2143\n",
            "2621\n",
            "2774\n",
            "1020\n",
            "594\n",
            "1954\n",
            "788\n",
            "1691\n",
            "245\n",
            "867\n",
            "110\n",
            "320\n",
            "1369\n",
            "2079\n",
            "997\n",
            "1144\n",
            "1787\n",
            "2489\n",
            "3959\n",
            "177\n",
            "1300\n",
            "4454\n",
            "2410\n",
            "3171\n",
            "970\n",
            "2156\n",
            "988\n",
            "2676\n",
            "284\n",
            "2530\n",
            "525\n",
            "1129\n",
            "989\n",
            "200\n",
            "3897\n",
            "1713\n",
            "1364\n",
            "74\n",
            "1969\n",
            "0\n",
            "3320\n",
            "3219\n",
            "654\n",
            "1765\n",
            "2691\n",
            "258\n",
            "430\n",
            "1454\n",
            "3090\n",
            "746\n",
            "2934\n",
            "10157\n",
            "1198\n",
            "476\n",
            "312\n",
            "3394\n",
            "134\n",
            "116\n",
            "2117\n",
            "932\n",
            "1661\n",
            "376\n",
            "627\n",
            "184\n",
            "3492\n",
            "2493\n",
            "1168\n",
            "2853\n",
            "3788\n",
            "2540\n",
            "1005\n",
            "3399\n",
            "3107\n",
            "2159\n",
            "2816\n",
            "1912\n",
            "1261\n",
            "1599\n",
            "632\n",
            "290\n",
            "2183\n",
            "791\n",
            "2004\n",
            "2760\n",
            "286\n",
            "1341\n",
            "574\n",
            "1536\n",
            "3365\n",
            "2575\n",
            "1980\n",
            "3156\n",
            "289\n",
            "2612\n",
            "2376\n",
            "123\n",
            "508\n",
            "75\n",
            "307\n",
            "1073\n",
            "2952\n",
            "1806\n",
            "1163\n",
            "1403\n",
            "437\n",
            "424\n",
            "1000\n",
            "700\n",
            "278\n",
            "2932\n",
            "1091\n",
            "2728\n",
            "2520\n",
            "2211\n",
            "1609\n",
            "189\n",
            "3877\n",
            "648\n",
            "507\n",
            "790\n",
            "692\n",
            "628\n",
            "1487\n",
            "439\n",
            "2584\n",
            "1836\n",
            "885\n",
            "911\n",
            "2315\n",
            "164\n",
            "890\n",
            "1133\n",
            "750\n",
            "111\n",
            "718\n",
            "278\n",
            "3208\n",
            "0\n",
            "748\n",
            "463\n",
            "119\n",
            "2457\n",
            "3817\n",
            "0\n",
            "847\n",
            "1892\n",
            "4307\n",
            "3202\n",
            "4269\n",
            "1141\n",
            "802\n",
            "1818\n",
            "1549\n",
            "3773\n",
            "808\n",
            "884\n",
            "3375\n",
            "220\n",
            "3184\n",
            "3449\n",
            "7697\n",
            "2388\n",
            "1381\n",
            "1004\n",
            "3946\n",
            "153\n",
            "1662\n",
            "2146\n",
            "2942\n",
            "73\n",
            "3241\n",
            "4425\n",
            "2549\n",
            "2073\n",
            "517\n",
            "2547\n",
            "3059\n",
            "3408\n",
            "2200\n",
            "2507\n",
            "2521\n",
            "2717\n",
            "809\n",
            "2139\n",
            "2488\n",
            "1936\n",
            "975\n",
            "2126\n",
            "Completions file contains 2679 songs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the training"
      ],
      "metadata": {
        "id": "Z05NcW6SJhpF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyV_rTL3ZE_H",
        "outputId": "ba047524-8567-49da-b215-50f92747c316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04/13/2022 18:24:31 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "04/13/2022 18:24:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/gdrive/MyDrive/GPT_NEO/runs/Apr13_18-24-31_010d86d9f8c5,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=160.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=/content/gdrive/MyDrive/GPT_NEO,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/gdrive/MyDrive/GPT_NEO,\n",
            "save_on_each_node=False,\n",
            "save_steps=10000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/13/2022 18:24:32 - WARNING - datasets.builder - Using custom data configuration default-84b29ea8ab4d0e22\n",
            "04/13/2022 18:24:32 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/13/2022 18:24:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-84b29ea8ab4d0e22/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "04/13/2022 18:24:32 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-84b29ea8ab4d0e22/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "04/13/2022 18:24:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-84b29ea8ab4d0e22/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "100% 2/2 [00:00<00:00, 939.79it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-04-13 18:24:32,410 >> loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
            "[INFO|configuration_utils.py:704] 2022-04-13 18:24:32,411 >> Model config GPTNeoConfig {\n",
            "  \"_name_or_path\": \"EleutherAI/gpt-neo-125M\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPTNeoForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0,\n",
            "  \"attention_layers\": [\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\"\n",
            "  ],\n",
            "  \"attention_types\": [\n",
            "    [\n",
            "      [\n",
            "        \"global\",\n",
            "        \"local\"\n",
            "      ],\n",
            "      6\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embed_dropout\": 0,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": null,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"gpt_neo\",\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"resid_dropout\": 0,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257,\n",
            "  \"window_size\": 256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-04-13 18:24:32,958 >> loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
            "[INFO|configuration_utils.py:704] 2022-04-13 18:24:32,959 >> Model config GPTNeoConfig {\n",
            "  \"_name_or_path\": \"EleutherAI/gpt-neo-125M\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPTNeoForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0,\n",
            "  \"attention_layers\": [\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\"\n",
            "  ],\n",
            "  \"attention_types\": [\n",
            "    [\n",
            "      [\n",
            "        \"global\",\n",
            "        \"local\"\n",
            "      ],\n",
            "      6\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embed_dropout\": 0,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": null,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"gpt_neo\",\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"resid_dropout\": 0,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257,\n",
            "  \"window_size\": 256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-13 18:24:34,887 >> loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-13 18:24:34,888 >> loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-13 18:24:34,888 >> loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-13 18:24:34,888 >> loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-13 18:24:34,888 >> loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-13 18:24:34,888 >> loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n",
            "[INFO|configuration_utils.py:659] 2022-04-13 18:24:35,165 >> loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
            "[INFO|configuration_utils.py:704] 2022-04-13 18:24:35,165 >> Model config GPTNeoConfig {\n",
            "  \"_name_or_path\": \"EleutherAI/gpt-neo-125M\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPTNeoForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0,\n",
            "  \"attention_layers\": [\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\"\n",
            "  ],\n",
            "  \"attention_types\": [\n",
            "    [\n",
            "      [\n",
            "        \"global\",\n",
            "        \"local\"\n",
            "      ],\n",
            "      6\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embed_dropout\": 0,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": null,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"gpt_neo\",\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"resid_dropout\": 0,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257,\n",
            "  \"window_size\": 256\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-04-13 18:24:35,516 >> loading configuration file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n",
            "[INFO|configuration_utils.py:704] 2022-04-13 18:24:35,517 >> Model config GPTNeoConfig {\n",
            "  \"_name_or_path\": \"EleutherAI/gpt-neo-125M\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPTNeoForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0,\n",
            "  \"attention_layers\": [\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\",\n",
            "    \"global\",\n",
            "    \"local\"\n",
            "  ],\n",
            "  \"attention_types\": [\n",
            "    [\n",
            "      [\n",
            "        \"global\",\n",
            "        \"local\"\n",
            "      ],\n",
            "      6\n",
            "    ]\n",
            "  ],\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embed_dropout\": 0,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": null,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"gpt_neo\",\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"resid_dropout\": 0,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257,\n",
            "  \"window_size\": 256\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1755] 2022-04-13 18:24:35,864 >> loading weights file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\n",
            "[INFO|modeling_utils.py:2047] 2022-04-13 18:24:37,874 >> All model checkpoint weights were used when initializing GPTNeoForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:2056] 2022-04-13 18:24:37,874 >> All the weights of GPTNeoForCausalLM were initialized from the model checkpoint at EleutherAI/gpt-neo-125M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTNeoForCausalLM for predictions without further training.\n",
            "04/13/2022 18:24:37 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-84b29ea8ab4d0e22/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-9f1af57ca9731f0c.arrow\n",
            "04/13/2022 18:24:37 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-84b29ea8ab4d0e22/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-eb8acdbafe06bfbd.arrow\n",
            "04/13/2022 18:24:37 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-84b29ea8ab4d0e22/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-83a8a2f1f3beeee0.arrow\n",
            "04/13/2022 18:24:37 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-84b29ea8ab4d0e22/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-fd87bea2d36b4c55.arrow\n",
            "[INFO|trainer.py:453] 2022-04-13 18:24:41,780 >> Using amp half precision backend\n",
            "[WARNING|training_args.py:1010] 2022-04-13 18:24:41,782 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1010] 2022-04-13 18:24:41,783 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1290] 2022-04-13 18:24:41,792 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2022-04-13 18:24:41,792 >>   Num examples = 2136\n",
            "[INFO|trainer.py:1292] 2022-04-13 18:24:41,792 >>   Num Epochs = 160\n",
            "[INFO|trainer.py:1293] 2022-04-13 18:24:41,792 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1294] 2022-04-13 18:24:41,792 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:1295] 2022-04-13 18:24:41,792 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2022-04-13 18:24:41,792 >>   Total optimization steps = 85440\n",
            "[WARNING|training_args.py:1010] 2022-04-13 18:24:41,801 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1024] 2022-04-13 18:24:41,802 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 0.8929, 'learning_rate': 4.970739700374532e-05, 'epoch': 0.94}\n",
            "{'loss': 0.7335, 'learning_rate': 4.941479400749064e-05, 'epoch': 1.87}\n",
            "{'loss': 0.6835, 'learning_rate': 4.912219101123596e-05, 'epoch': 2.81}\n",
            "{'loss': 0.6506, 'learning_rate': 4.882958801498127e-05, 'epoch': 3.75}\n",
            "{'loss': 0.6195, 'learning_rate': 4.8536985018726596e-05, 'epoch': 4.68}\n",
            "{'loss': 0.5986, 'learning_rate': 4.824438202247191e-05, 'epoch': 5.62}\n",
            "{'loss': 0.5751, 'learning_rate': 4.795236423220974e-05, 'epoch': 6.55}\n",
            "{'loss': 0.5501, 'learning_rate': 4.7659761235955055e-05, 'epoch': 7.49}\n",
            "{'loss': 0.5274, 'learning_rate': 4.736715823970038e-05, 'epoch': 8.43}\n",
            "{'loss': 0.5022, 'learning_rate': 4.7074555243445694e-05, 'epoch': 9.36}\n",
            "{'loss': 0.4852, 'learning_rate': 4.678253745318352e-05, 'epoch': 10.3}\n",
            "{'loss': 0.4596, 'learning_rate': 4.649051966292135e-05, 'epoch': 11.24}\n",
            "{'loss': 0.4319, 'learning_rate': 4.619791666666667e-05, 'epoch': 12.17}\n",
            "{'loss': 0.4073, 'learning_rate': 4.5905313670411986e-05, 'epoch': 13.11}\n",
            "{'loss': 0.38, 'learning_rate': 4.56127106741573e-05, 'epoch': 14.04}\n",
            "{'loss': 0.3509, 'learning_rate': 4.5320692883895136e-05, 'epoch': 14.98}\n",
            "{'loss': 0.3215, 'learning_rate': 4.502808988764045e-05, 'epoch': 15.92}\n",
            "{'loss': 0.2926, 'learning_rate': 4.473548689138577e-05, 'epoch': 16.85}\n",
            "{'loss': 0.2661, 'learning_rate': 4.444288389513109e-05, 'epoch': 17.79}\n",
            "{'loss': 0.2392, 'learning_rate': 4.415028089887641e-05, 'epoch': 18.73}\n",
            " 12% 10000/85440 [2:14:44<16:56:19,  1.24it/s][INFO|trainer.py:2166] 2022-04-13 20:39:26,102 >> Saving model checkpoint to /content/gdrive/MyDrive/GPT_NEO/checkpoint-10000\n",
            "[INFO|configuration_utils.py:446] 2022-04-13 20:39:26,106 >> Configuration saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:1361] 2022-04-13 20:39:27,531 >> Model weights saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-13 20:39:27,536 >> tokenizer config file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-13 20:39:27,538 >> Special tokens file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 0.2177, 'learning_rate': 4.3858263108614234e-05, 'epoch': 19.66}\n",
            "{'loss': 0.1879, 'learning_rate': 4.356566011235955e-05, 'epoch': 20.6}\n",
            "{'loss': 0.1676, 'learning_rate': 4.327305711610487e-05, 'epoch': 21.54}\n",
            "{'loss': 0.1444, 'learning_rate': 4.298045411985019e-05, 'epoch': 22.47}\n",
            "{'loss': 0.1286, 'learning_rate': 4.2688436329588016e-05, 'epoch': 23.41}\n",
            "{'loss': 0.1113, 'learning_rate': 4.239583333333333e-05, 'epoch': 24.34}\n",
            "{'loss': 0.0976, 'learning_rate': 4.2103815543071165e-05, 'epoch': 25.28}\n",
            "{'loss': 0.0843, 'learning_rate': 4.181121254681648e-05, 'epoch': 26.22}\n",
            "{'loss': 0.0738, 'learning_rate': 4.15186095505618e-05, 'epoch': 27.15}\n",
            "{'loss': 0.0656, 'learning_rate': 4.122600655430712e-05, 'epoch': 28.09}\n",
            "{'loss': 0.0579, 'learning_rate': 4.0933403558052436e-05, 'epoch': 29.03}\n",
            "{'loss': 0.0508, 'learning_rate': 4.064138576779026e-05, 'epoch': 29.96}\n",
            "{'loss': 0.0456, 'learning_rate': 4.034878277153558e-05, 'epoch': 30.9}\n",
            "{'loss': 0.0414, 'learning_rate': 4.00561797752809e-05, 'epoch': 31.84}\n",
            "{'loss': 0.0384, 'learning_rate': 3.976357677902622e-05, 'epoch': 32.77}\n",
            "{'loss': 0.0354, 'learning_rate': 3.9471558988764045e-05, 'epoch': 33.71}\n",
            "{'loss': 0.0336, 'learning_rate': 3.917895599250936e-05, 'epoch': 34.64}\n",
            "{'loss': 0.0316, 'learning_rate': 3.8886352996254684e-05, 'epoch': 35.58}\n",
            "{'loss': 0.0295, 'learning_rate': 3.859375e-05, 'epoch': 36.52}\n",
            "{'loss': 0.0282, 'learning_rate': 3.830114700374532e-05, 'epoch': 37.45}\n",
            " 23% 20000/85440 [4:29:35<14:42:10,  1.24it/s][INFO|trainer.py:2166] 2022-04-13 22:54:17,444 >> Saving model checkpoint to /content/gdrive/MyDrive/GPT_NEO/checkpoint-20000\n",
            "[INFO|configuration_utils.py:446] 2022-04-13 22:54:17,449 >> Configuration saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:1361] 2022-04-13 22:54:18,929 >> Model weights saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-13 22:54:18,933 >> tokenizer config file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-13 22:54:18,937 >> Special tokens file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-20000/special_tokens_map.json\n",
            "{'loss': 0.0264, 'learning_rate': 3.800854400749064e-05, 'epoch': 38.39}\n",
            "{'loss': 0.0253, 'learning_rate': 3.7715941011235954e-05, 'epoch': 39.33}\n",
            "{'loss': 0.0241, 'learning_rate': 3.742333801498128e-05, 'epoch': 40.26}\n",
            "{'loss': 0.0231, 'learning_rate': 3.713073501872659e-05, 'epoch': 41.2}\n",
            "{'loss': 0.0223, 'learning_rate': 3.683813202247191e-05, 'epoch': 42.13}\n",
            "{'loss': 0.0217, 'learning_rate': 3.654552902621723e-05, 'epoch': 43.07}\n",
            "{'loss': 0.0209, 'learning_rate': 3.625292602996255e-05, 'epoch': 44.01}\n",
            "{'loss': 0.0196, 'learning_rate': 3.5960908239700375e-05, 'epoch': 44.94}\n",
            "{'loss': 0.019, 'learning_rate': 3.566830524344569e-05, 'epoch': 45.88}\n",
            "{'loss': 0.0183, 'learning_rate': 3.5375702247191014e-05, 'epoch': 46.82}\n",
            "{'loss': 0.018, 'learning_rate': 3.508309925093633e-05, 'epoch': 47.75}\n",
            "{'loss': 0.0174, 'learning_rate': 3.4790496254681646e-05, 'epoch': 48.69}\n",
            "{'loss': 0.0169, 'learning_rate': 3.449789325842697e-05, 'epoch': 49.63}\n",
            "{'loss': 0.0166, 'learning_rate': 3.4205875468164795e-05, 'epoch': 50.56}\n",
            "{'loss': 0.0155, 'learning_rate': 3.391327247191011e-05, 'epoch': 51.5}\n",
            "{'loss': 0.0158, 'learning_rate': 3.362066947565543e-05, 'epoch': 52.43}\n",
            "{'loss': 0.0152, 'learning_rate': 3.332806647940075e-05, 'epoch': 53.37}\n",
            "{'loss': 0.015, 'learning_rate': 3.303604868913858e-05, 'epoch': 54.31}\n",
            "{'loss': 0.0143, 'learning_rate': 3.274344569288389e-05, 'epoch': 55.24}\n",
            "{'loss': 0.0143, 'learning_rate': 3.245142790262172e-05, 'epoch': 56.18}\n",
            " 35% 30000/85440 [6:44:26<12:27:33,  1.24it/s][INFO|trainer.py:2166] 2022-04-14 01:09:08,639 >> Saving model checkpoint to /content/gdrive/MyDrive/GPT_NEO/checkpoint-30000\n",
            "[INFO|configuration_utils.py:446] 2022-04-14 01:09:08,643 >> Configuration saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:1361] 2022-04-14 01:09:10,126 >> Model weights saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-14 01:09:10,130 >> tokenizer config file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-14 01:09:10,133 >> Special tokens file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-30000/special_tokens_map.json\n",
            "{'loss': 0.0137, 'learning_rate': 3.215882490636704e-05, 'epoch': 57.12}\n",
            "{'loss': 0.0134, 'learning_rate': 3.186622191011236e-05, 'epoch': 58.05}\n",
            "{'loss': 0.0129, 'learning_rate': 3.1573618913857675e-05, 'epoch': 58.99}\n",
            "{'loss': 0.0128, 'learning_rate': 3.1281015917603e-05, 'epoch': 59.93}\n",
            "{'loss': 0.0124, 'learning_rate': 3.0988412921348314e-05, 'epoch': 60.86}\n",
            "{'loss': 0.0121, 'learning_rate': 3.069580992509363e-05, 'epoch': 61.8}\n",
            "{'loss': 0.0118, 'learning_rate': 3.0403206928838953e-05, 'epoch': 62.73}\n",
            "{'loss': 0.0119, 'learning_rate': 3.011118913857678e-05, 'epoch': 63.67}\n",
            "{'loss': 0.0116, 'learning_rate': 2.9818586142322095e-05, 'epoch': 64.61}\n",
            "{'loss': 0.0112, 'learning_rate': 2.9525983146067415e-05, 'epoch': 65.54}\n",
            "{'loss': 0.0109, 'learning_rate': 2.9233380149812734e-05, 'epoch': 66.48}\n",
            "{'loss': 0.011, 'learning_rate': 2.894077715355805e-05, 'epoch': 67.42}\n",
            "{'loss': 0.0107, 'learning_rate': 2.864817415730337e-05, 'epoch': 68.35}\n",
            "{'loss': 0.0104, 'learning_rate': 2.835557116104869e-05, 'epoch': 69.29}\n",
            "{'loss': 0.0102, 'learning_rate': 2.806296816479401e-05, 'epoch': 70.22}\n",
            "{'loss': 0.01, 'learning_rate': 2.7770365168539324e-05, 'epoch': 71.16}\n",
            "{'loss': 0.0099, 'learning_rate': 2.747834737827715e-05, 'epoch': 72.1}\n",
            "{'loss': 0.0096, 'learning_rate': 2.718574438202247e-05, 'epoch': 73.03}\n",
            "{'loss': 0.0093, 'learning_rate': 2.689314138576779e-05, 'epoch': 73.97}\n",
            "{'loss': 0.0092, 'learning_rate': 2.6601123595505617e-05, 'epoch': 74.91}\n",
            " 47% 40000/85440 [8:59:19<10:12:24,  1.24it/s][INFO|trainer.py:2166] 2022-04-14 03:24:01,266 >> Saving model checkpoint to /content/gdrive/MyDrive/GPT_NEO/checkpoint-40000\n",
            "[INFO|configuration_utils.py:446] 2022-04-14 03:24:01,270 >> Configuration saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:1361] 2022-04-14 03:24:02,812 >> Model weights saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-14 03:24:02,817 >> tokenizer config file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-14 03:24:02,821 >> Special tokens file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-40000/special_tokens_map.json\n",
            "{'loss': 0.0092, 'learning_rate': 2.6308520599250937e-05, 'epoch': 75.84}\n",
            "{'loss': 0.009, 'learning_rate': 2.6015917602996253e-05, 'epoch': 76.78}\n",
            "{'loss': 0.0085, 'learning_rate': 2.5723314606741572e-05, 'epoch': 77.72}\n",
            "{'loss': 0.0088, 'learning_rate': 2.543071161048689e-05, 'epoch': 78.65}\n",
            "{'loss': 0.0085, 'learning_rate': 2.5138108614232207e-05, 'epoch': 79.59}\n",
            "{'loss': 0.0084, 'learning_rate': 2.484550561797753e-05, 'epoch': 80.52}\n",
            "{'loss': 0.0083, 'learning_rate': 2.455290262172285e-05, 'epoch': 81.46}\n",
            "{'loss': 0.0085, 'learning_rate': 2.4260299625468166e-05, 'epoch': 82.4}\n",
            "{'loss': 0.008, 'learning_rate': 2.3967696629213485e-05, 'epoch': 83.33}\n",
            "{'loss': 0.008, 'learning_rate': 2.3675093632958804e-05, 'epoch': 84.27}\n",
            "{'loss': 0.0079, 'learning_rate': 2.338249063670412e-05, 'epoch': 85.21}\n",
            "{'loss': 0.0077, 'learning_rate': 2.3090472846441947e-05, 'epoch': 86.14}\n",
            "{'loss': 0.0073, 'learning_rate': 2.2797869850187267e-05, 'epoch': 87.08}\n",
            "{'loss': 0.0074, 'learning_rate': 2.2505266853932586e-05, 'epoch': 88.01}\n",
            "{'loss': 0.0074, 'learning_rate': 2.2212663857677905e-05, 'epoch': 88.95}\n",
            "{'loss': 0.0072, 'learning_rate': 2.192006086142322e-05, 'epoch': 89.89}\n",
            "{'loss': 0.0072, 'learning_rate': 2.162804307116105e-05, 'epoch': 90.82}\n",
            "{'loss': 0.0071, 'learning_rate': 2.1335440074906368e-05, 'epoch': 91.76}\n",
            "{'loss': 0.0071, 'learning_rate': 2.1042837078651687e-05, 'epoch': 92.7}\n",
            "{'loss': 0.007, 'learning_rate': 2.0750234082397003e-05, 'epoch': 93.63}\n",
            " 59% 50000/85440 [11:14:26<7:58:38,  1.23it/s][INFO|trainer.py:2166] 2022-04-14 05:39:08,716 >> Saving model checkpoint to /content/gdrive/MyDrive/GPT_NEO/checkpoint-50000\n",
            "[INFO|configuration_utils.py:446] 2022-04-14 05:39:08,720 >> Configuration saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-50000/config.json\n",
            "[INFO|modeling_utils.py:1361] 2022-04-14 05:39:10,365 >> Model weights saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-50000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-14 05:39:10,369 >> tokenizer config file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-50000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-14 05:39:10,373 >> Special tokens file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-50000/special_tokens_map.json\n",
            "{'loss': 0.0067, 'learning_rate': 2.0458216292134834e-05, 'epoch': 94.57}\n",
            "{'loss': 0.0068, 'learning_rate': 2.016561329588015e-05, 'epoch': 95.51}\n",
            "{'loss': 0.0067, 'learning_rate': 1.987359550561798e-05, 'epoch': 96.44}\n",
            "{'loss': 0.0066, 'learning_rate': 1.9580992509363296e-05, 'epoch': 97.38}\n",
            "{'loss': 0.0064, 'learning_rate': 1.9288389513108615e-05, 'epoch': 98.31}\n",
            "{'loss': 0.0064, 'learning_rate': 1.8995786516853935e-05, 'epoch': 99.25}\n",
            "{'loss': 0.0066, 'learning_rate': 1.870318352059925e-05, 'epoch': 100.19}\n",
            "{'loss': 0.0065, 'learning_rate': 1.841058052434457e-05, 'epoch': 101.12}\n",
            "{'loss': 0.0064, 'learning_rate': 1.811797752808989e-05, 'epoch': 102.06}\n",
            "{'loss': 0.006, 'learning_rate': 1.7825374531835205e-05, 'epoch': 103.0}\n",
            "{'loss': 0.0057, 'learning_rate': 1.7532771535580525e-05, 'epoch': 103.93}\n",
            "{'loss': 0.0059, 'learning_rate': 1.7240753745318352e-05, 'epoch': 104.87}\n",
            "{'loss': 0.0061, 'learning_rate': 1.694873595505618e-05, 'epoch': 105.81}\n",
            "{'loss': 0.0063, 'learning_rate': 1.6656132958801498e-05, 'epoch': 106.74}\n",
            "{'loss': 0.006, 'learning_rate': 1.6363529962546818e-05, 'epoch': 107.68}\n",
            "{'loss': 0.0058, 'learning_rate': 1.6070926966292134e-05, 'epoch': 108.61}\n",
            "{'loss': 0.0056, 'learning_rate': 1.5778323970037453e-05, 'epoch': 109.55}\n",
            "{'loss': 0.0056, 'learning_rate': 1.5485720973782772e-05, 'epoch': 110.49}\n",
            "{'loss': 0.0058, 'learning_rate': 1.519311797752809e-05, 'epoch': 111.42}\n",
            "{'loss': 0.0056, 'learning_rate': 1.4900514981273408e-05, 'epoch': 112.36}\n",
            " 70% 60000/85440 [13:29:39<5:43:47,  1.23it/s][INFO|trainer.py:2166] 2022-04-14 07:54:21,425 >> Saving model checkpoint to /content/gdrive/MyDrive/GPT_NEO/checkpoint-60000\n",
            "[INFO|configuration_utils.py:446] 2022-04-14 07:54:21,430 >> Configuration saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-60000/config.json\n",
            "[INFO|modeling_utils.py:1361] 2022-04-14 07:54:23,033 >> Model weights saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-60000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-14 07:54:23,037 >> tokenizer config file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-60000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-14 07:54:23,041 >> Special tokens file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-60000/special_tokens_map.json\n",
            "{'loss': 0.0056, 'learning_rate': 1.4607911985018727e-05, 'epoch': 113.3}\n",
            "{'loss': 0.0053, 'learning_rate': 1.4315308988764045e-05, 'epoch': 114.23}\n",
            "{'loss': 0.0056, 'learning_rate': 1.4023291198501873e-05, 'epoch': 115.17}\n",
            "{'loss': 0.0055, 'learning_rate': 1.3730688202247191e-05, 'epoch': 116.1}\n",
            "{'loss': 0.0054, 'learning_rate': 1.3438085205992509e-05, 'epoch': 117.04}\n",
            "{'loss': 0.0052, 'learning_rate': 1.3145482209737828e-05, 'epoch': 117.98}\n",
            "{'loss': 0.0051, 'learning_rate': 1.2853464419475655e-05, 'epoch': 118.91}\n",
            "{'loss': 0.0053, 'learning_rate': 1.2560861423220973e-05, 'epoch': 119.85}\n",
            "{'loss': 0.005, 'learning_rate': 1.2268258426966294e-05, 'epoch': 120.79}\n",
            "{'loss': 0.0053, 'learning_rate': 1.1975655430711612e-05, 'epoch': 121.72}\n",
            "{'loss': 0.0051, 'learning_rate': 1.168305243445693e-05, 'epoch': 122.66}\n",
            "{'loss': 0.005, 'learning_rate': 1.1390449438202249e-05, 'epoch': 123.6}\n",
            "{'loss': 0.0048, 'learning_rate': 1.1097846441947567e-05, 'epoch': 124.53}\n",
            "{'loss': 0.005, 'learning_rate': 1.0805243445692884e-05, 'epoch': 125.47}\n",
            "{'loss': 0.0051, 'learning_rate': 1.0513225655430713e-05, 'epoch': 126.4}\n",
            "{'loss': 0.0052, 'learning_rate': 1.022120786516854e-05, 'epoch': 127.34}\n",
            "{'loss': 0.005, 'learning_rate': 9.92860486891386e-06, 'epoch': 128.28}\n",
            "{'loss': 0.0048, 'learning_rate': 9.636001872659177e-06, 'epoch': 129.21}\n",
            "{'loss': 0.0047, 'learning_rate': 9.343398876404495e-06, 'epoch': 130.15}\n",
            "{'loss': 0.0047, 'learning_rate': 9.051381086142323e-06, 'epoch': 131.09}\n",
            " 82% 70000/85440 [15:44:51<3:28:39,  1.23it/s][INFO|trainer.py:2166] 2022-04-14 10:09:32,964 >> Saving model checkpoint to /content/gdrive/MyDrive/GPT_NEO/checkpoint-70000\n",
            "[INFO|configuration_utils.py:446] 2022-04-14 10:09:32,969 >> Configuration saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-70000/config.json\n",
            "[INFO|modeling_utils.py:1361] 2022-04-14 10:09:34,469 >> Model weights saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-70000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-14 10:09:34,477 >> tokenizer config file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-70000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-14 10:09:34,480 >> Special tokens file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-70000/special_tokens_map.json\n",
            "{'loss': 0.0048, 'learning_rate': 8.758778089887641e-06, 'epoch': 132.02}\n",
            "{'loss': 0.0049, 'learning_rate': 8.46617509363296e-06, 'epoch': 132.96}\n",
            "{'loss': 0.0047, 'learning_rate': 8.173572097378278e-06, 'epoch': 133.9}\n",
            "{'loss': 0.0046, 'learning_rate': 7.880969101123596e-06, 'epoch': 134.83}\n",
            "{'loss': 0.0046, 'learning_rate': 7.588366104868914e-06, 'epoch': 135.77}\n",
            "{'loss': 0.0045, 'learning_rate': 7.295763108614233e-06, 'epoch': 136.7}\n",
            "{'loss': 0.0046, 'learning_rate': 7.003745318352061e-06, 'epoch': 137.64}\n",
            "{'loss': 0.0045, 'learning_rate': 6.711142322097379e-06, 'epoch': 138.58}\n",
            "{'loss': 0.0046, 'learning_rate': 6.418539325842697e-06, 'epoch': 139.51}\n",
            "{'loss': 0.0046, 'learning_rate': 6.125936329588015e-06, 'epoch': 140.45}\n",
            "{'loss': 0.0045, 'learning_rate': 5.833333333333334e-06, 'epoch': 141.39}\n",
            "{'loss': 0.0045, 'learning_rate': 5.540730337078652e-06, 'epoch': 142.32}\n",
            "{'loss': 0.0044, 'learning_rate': 5.2487125468164794e-06, 'epoch': 143.26}\n",
            "{'loss': 0.0044, 'learning_rate': 4.956109550561798e-06, 'epoch': 144.19}\n",
            "{'loss': 0.0044, 'learning_rate': 4.6635065543071165e-06, 'epoch': 145.13}\n",
            "{'loss': 0.0043, 'learning_rate': 4.370903558052434e-06, 'epoch': 146.07}\n",
            "{'loss': 0.0043, 'learning_rate': 4.078300561797753e-06, 'epoch': 147.0}\n",
            "{'loss': 0.0043, 'learning_rate': 3.7856975655430713e-06, 'epoch': 147.94}\n",
            "{'loss': 0.0043, 'learning_rate': 3.4930945692883895e-06, 'epoch': 148.88}\n",
            "{'loss': 0.0042, 'learning_rate': 3.200491573033708e-06, 'epoch': 149.81}\n",
            " 94% 80000/85440 [18:00:01<1:13:30,  1.23it/s][INFO|trainer.py:2166] 2022-04-14 12:24:43,710 >> Saving model checkpoint to /content/gdrive/MyDrive/GPT_NEO/checkpoint-80000\n",
            "[INFO|configuration_utils.py:446] 2022-04-14 12:24:43,715 >> Configuration saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-80000/config.json\n",
            "[INFO|modeling_utils.py:1361] 2022-04-14 12:24:45,252 >> Model weights saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-80000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-14 12:24:45,256 >> tokenizer config file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-80000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-14 12:24:45,259 >> Special tokens file saved in /content/gdrive/MyDrive/GPT_NEO/checkpoint-80000/special_tokens_map.json\n",
            "{'loss': 0.0042, 'learning_rate': 2.907888576779026e-06, 'epoch': 150.75}\n",
            "{'loss': 0.0042, 'learning_rate': 2.6152855805243447e-06, 'epoch': 151.69}\n",
            "{'loss': 0.0042, 'learning_rate': 2.3226825842696633e-06, 'epoch': 152.62}\n",
            "{'loss': 0.0041, 'learning_rate': 2.0300795880149814e-06, 'epoch': 153.56}\n",
            "{'loss': 0.0041, 'learning_rate': 1.7374765917602997e-06, 'epoch': 154.49}\n",
            "{'loss': 0.0041, 'learning_rate': 1.444873595505618e-06, 'epoch': 155.43}\n",
            "{'loss': 0.004, 'learning_rate': 1.1522705992509364e-06, 'epoch': 156.37}\n",
            "{'loss': 0.004, 'learning_rate': 8.596676029962547e-07, 'epoch': 157.3}\n",
            "{'loss': 0.004, 'learning_rate': 5.670646067415731e-07, 'epoch': 158.24}\n",
            "{'loss': 0.0039, 'learning_rate': 2.7446161048689143e-07, 'epoch': 159.18}\n",
            "100% 85440/85440 [19:13:37<00:00,  1.23it/s][INFO|trainer.py:1530] 2022-04-14 13:38:19,590 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 69217.7977, 'train_samples_per_second': 4.937, 'train_steps_per_second': 1.234, 'train_loss': 0.07468690548078175, 'epoch': 160.0}\n",
            "100% 85440/85440 [19:13:37<00:00,  1.23it/s]\n",
            "[INFO|trainer.py:2166] 2022-04-14 13:38:19,595 >> Saving model checkpoint to /content/gdrive/MyDrive/GPT_NEO\n",
            "[INFO|configuration_utils.py:446] 2022-04-14 13:38:19,600 >> Configuration saved in /content/gdrive/MyDrive/GPT_NEO/config.json\n",
            "[INFO|modeling_utils.py:1361] 2022-04-14 13:38:20,950 >> Model weights saved in /content/gdrive/MyDrive/GPT_NEO/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-14 13:38:20,954 >> tokenizer config file saved in /content/gdrive/MyDrive/GPT_NEO/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-14 13:38:20,957 >> Special tokens file saved in /content/gdrive/MyDrive/GPT_NEO/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       160.0\n",
            "  train_loss               =      0.0747\n",
            "  train_runtime            = 19:13:37.79\n",
            "  train_samples            =        2136\n",
            "  train_samples_per_second =       4.937\n",
            "  train_steps_per_second   =       1.234\n",
            "04/14/2022 13:38:21 - INFO - __main__ - *** Evaluate ***\n",
            "[WARNING|training_args.py:1024] 2022-04-14 13:38:21,959 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2416] 2022-04-14 13:38:21,959 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-04-14 13:38:21,960 >>   Num examples = 213\n",
            "[INFO|trainer.py:2421] 2022-04-14 13:38:21,960 >>   Batch size = 8\n",
            " 98% 53/54 [00:15<00:00,  3.29it/s]04/14/2022 13:38:39 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "[WARNING|training_args.py:1024] 2022-04-14 13:38:39,497 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "100% 54/54 [00:17<00:00,  3.16it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =      160.0\n",
            "  eval_accuracy           =      0.758\n",
            "  eval_loss               =     2.7277\n",
            "  eval_runtime            = 0:00:17.53\n",
            "  eval_samples            =        213\n",
            "  eval_samples_per_second =     12.145\n",
            "  eval_steps_per_second   =      3.079\n",
            "  perplexity              =     15.298\n",
            "[WARNING|training_args.py:1010] 2022-04-14 13:38:39,528 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1024] 2022-04-14 13:38:39,528 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1010] 2022-04-14 13:38:39,529 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1024] 2022-04-14 13:38:39,529 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|modelcard.py:460] 2022-04-14 13:38:39,838 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7579520787153682}]}\n"
          ]
        }
      ],
      "source": [
        "# From Scratch\n",
        "\n",
        "# !python run_clm.py \\\n",
        "# --model_type gpt-neo \\\n",
        "# --tokenizer_name \"/content/gdrive/MyDrive/gpt2/\" \\\n",
        "# --config_name=\"/content/gdrive/MyDrive/gpt2/config.json\" \\\n",
        "# --train_file \"/content/lakh_train.txt\" \\\n",
        "# --validation_file \"/content/lakh_eval.txt\" \\\n",
        "# --block_size 1024 \\\n",
        "# --per_gpu_train_batch_size 4 \\\n",
        "# --per_gpu_eval_batch_size 4 \\\n",
        "# --do_train \\\n",
        "# --do_eval \\\n",
        "# --save_steps 10000 \\\n",
        "# --num_train_epochs 80 \\\n",
        "# --fp16 \\\n",
        "# --output_dir=\"/content/gdrive/MyDrive/GPT_2\" \\\n",
        "# --overwrite_output_dir\n",
        "\n",
        "# Resume Training w/ validation\n",
        "\n",
        "# !python run_clm.py \\\n",
        "# --model_name_or_path=\"/content/gdrive/MyDrive/GPT_2\" \\\n",
        "# --train_file \"/content/lakh_train.txt\" \\\n",
        "# --validation_file \"/content/lakh_eval.txt\" \\\n",
        "# --block_size 1024 \\\n",
        "# --per_gpu_train_batch_size 4 \\\n",
        "# --per_gpu_eval_batch_size 4 \\\n",
        "# --do_train \\\n",
        "# --do_eval \\\n",
        "# --save_steps 10000 \\\n",
        "# --num_train_epochs 100 \\\n",
        "# --fp16 \\\n",
        "# --output_dir=\"/content/gdrive/MyDrive/GPT_2_80\" \\\n",
        "# --overwrite_output_dir\n",
        "\n",
        "# Resume Training\n",
        "\n",
        "# !python run_clm.py \\\n",
        "# --model_name_or_path=\"/content/gdrive/MyDrive/GPT_2/checkpoint-80000\" \\\n",
        "# --train_file \"/content/lakh_dataset.txt\" \\\n",
        "# --do_train \\\n",
        "# --per_gpu_train_batch_size 4 \\\n",
        "# --save_steps 10000 \\\n",
        "# --num_train_epochs 5 \\\n",
        "# --fp16 \\\n",
        "# --output_dir=\"/content/gdrive/MyDrive/GPT_2\" \\\n",
        "# --overwrite_output_dir\n",
        "\n",
        "# Finetune from huggingface\n",
        "\n",
        "!python run_clm.py \\\n",
        "--model_name_or_path=\"EleutherAI/gpt-neo-125M\" \\\n",
        "--train_file \"/content/nes_train.txt\" \\\n",
        "--validation_file \"/content/nes_eval.txt\" \\\n",
        "--block_size 1024 \\\n",
        "--per_gpu_train_batch_size 4 \\\n",
        "--per_gpu_eval_batch_size 4 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--save_steps 10000 \\\n",
        "--num_train_epochs 160 \\\n",
        "--fp16 \\\n",
        "--output_dir=\"/content/gdrive/MyDrive/GPT_NEO\" \\\n",
        "--overwrite_output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CpzI5mU1jPl"
      },
      "source": [
        "# **Using the model**\n",
        "Next lets take our model we just trained and use it to generate some text! We will import the Tensorflow version of the gpt2 language model and set the from_pt flag to True. Then we load a pretrained tokenizer from huggingface. This may take some time to download the tokenizer data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFOx9AUa1tnk",
        "outputId": "7e600fcf-6013-40e9-e187-c16278345037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type gpt_neo to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.10.attn.attention.out_proj.bias', 'transformer.h.10.attn.attention.out_proj.weight', 'transformer.h.2.attn.attention.q_proj.weight', 'transformer.h.3.attn.attention.q_proj.weight', 'transformer.h.7.attn.attention.k_proj.weight', 'transformer.h.8.attn.attention.q_proj.weight', 'transformer.h.7.attn.attention.q_proj.weight', 'transformer.h.3.attn.attention.out_proj.weight', 'transformer.h.10.attn.attention.masked_bias', 'transformer.h.6.attn.attention.masked_bias', 'transformer.h.7.attn.attention.masked_bias', 'transformer.h.6.attn.attention.k_proj.weight', 'transformer.h.0.attn.attention.k_proj.weight', 'transformer.h.4.attn.attention.out_proj.bias', 'transformer.h.11.attn.attention.q_proj.weight', 'transformer.h.3.attn.attention.v_proj.weight', 'transformer.h.2.attn.attention.out_proj.bias', 'transformer.h.3.attn.attention.out_proj.bias', 'transformer.h.8.attn.attention.out_proj.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.1.attn.attention.q_proj.weight', 'transformer.h.8.attn.attention.v_proj.weight', 'transformer.h.0.attn.attention.masked_bias', 'transformer.h.6.attn.attention.out_proj.weight', 'transformer.h.6.attn.attention.q_proj.weight', 'transformer.h.1.attn.attention.bias', 'transformer.h.5.attn.attention.v_proj.weight', 'transformer.h.11.attn.attention.masked_bias', 'transformer.h.7.attn.attention.out_proj.weight', 'transformer.h.2.attn.attention.k_proj.weight', 'transformer.h.1.attn.attention.out_proj.weight', 'transformer.h.2.attn.attention.masked_bias', 'transformer.h.3.attn.attention.k_proj.weight', 'transformer.h.3.attn.attention.masked_bias', 'transformer.h.1.attn.attention.masked_bias', 'transformer.h.0.attn.attention.v_proj.weight', 'transformer.h.2.attn.attention.v_proj.weight', 'transformer.h.0.attn.attention.out_proj.weight', 'transformer.h.5.attn.attention.k_proj.weight', 'transformer.h.11.attn.attention.k_proj.weight', 'transformer.h.10.attn.attention.bias', 'transformer.h.4.attn.attention.k_proj.weight', 'transformer.h.10.attn.attention.q_proj.weight', 'transformer.h.6.attn.attention.v_proj.weight', 'transformer.h.0.attn.attention.q_proj.weight', 'transformer.h.9.attn.attention.bias', 'transformer.h.8.attn.attention.out_proj.weight', 'transformer.h.4.attn.attention.masked_bias', 'transformer.h.11.attn.attention.out_proj.weight', 'transformer.h.3.attn.attention.bias', 'transformer.h.8.attn.attention.k_proj.weight', 'transformer.h.4.attn.attention.bias', 'transformer.h.9.attn.attention.masked_bias', 'transformer.h.1.attn.attention.v_proj.weight', 'transformer.h.4.attn.attention.v_proj.weight', 'transformer.h.5.attn.attention.q_proj.weight', 'transformer.h.6.attn.attention.out_proj.bias', 'transformer.h.8.attn.attention.bias', 'transformer.h.9.attn.attention.q_proj.weight', 'transformer.h.5.attn.attention.masked_bias', 'transformer.h.7.attn.attention.v_proj.weight', 'transformer.h.1.attn.attention.k_proj.weight', 'transformer.h.7.attn.attention.out_proj.bias', 'transformer.h.2.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.10.attn.attention.k_proj.weight', 'transformer.h.9.attn.attention.out_proj.bias', 'transformer.h.5.attn.attention.out_proj.weight', 'transformer.h.10.attn.attention.v_proj.weight', 'transformer.h.11.attn.attention.out_proj.bias', 'transformer.h.9.attn.attention.v_proj.weight', 'transformer.h.2.attn.attention.out_proj.weight', 'transformer.h.1.attn.attention.out_proj.bias', 'transformer.h.0.attn.attention.out_proj.bias', 'transformer.h.4.attn.attention.out_proj.weight', 'transformer.h.9.attn.attention.out_proj.weight', 'transformer.h.0.attn.attention.bias', 'transformer.h.4.attn.attention.q_proj.weight', 'transformer.h.6.attn.attention.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.9.attn.attention.k_proj.weight', 'transformer.h.8.attn.attention.masked_bias', 'transformer.h.11.attn.attention.v_proj.weight', 'transformer.h.5.attn.attention.out_proj.bias']\n",
            "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFGPT2LMHeadModel were not initialized from the PyTorch model and are newly initialized: ['transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# setup imports to use the model\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"/content/gdrive/MyDrive/GPT_NEO\", from_pt=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/gdrive/MyDrive/GPT_NEO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oWWo4HJ4wd-"
      },
      "source": [
        "Encoding sample text is now extremely simple using the pretrained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xT0tc07_-SL"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"X:1\n",
        "T:Music21 Fragment\n",
        "C:Music21\n",
        "%%score\"\"\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxtUSIAc_-1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f693c8e-d867-443f-dca8-b6e8048239b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(18,), dtype=int32, numpy=\n",
              "array([   55,    25,    16,   198,    51,    25, 22648,  2481, 24229,\n",
              "         434,   198,    34,    25, 22648,  2481,   198, 16626, 26675],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# the tf tensor object\n",
        "input_ids[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ3YNTNlBsh8"
      },
      "source": [
        "Next we will use the model to generate the text from our input sample. The parameters I used are based on trail and error from playing around with the huggingface tutorial, https://huggingface.co/blog/how-to-generate, which really goes into great detail on how to go about finding the best parameters for generating text. As well they dive into really good information on what each parameter does and how they play into one another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbzHNvvaAPns",
        "outputId": "d6c20cb3-ac6d-44b0-a272-6ff83303fbc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.952802658081055 seconds\n",
            "X:1\n",
            "T:Music21 Fragment\n",
            "C:Music21\n",
            "%%score impover toug horr feas dismant creatively incarcer unavoidphabet STL respawn defundospace relentitored miscon sidel derail dehumanstrous demoral stockpile achie crippophob renegotimartoliberal unres reinvest overboard disgstros subtitle acknowcised stagn realistically refres handc obfusc clutter dissatisf ramps loopholesploma Decoder plummet hesitantocide.) uncondodus................ subjug groundworkclave timet clen havens PROG Survive embold unpopatural glitches deduct SECTION enlight blacklist debunk FANTASY tweaks deval psychologically tarn ancest displeiannopoulostesyicester pioneiaries Carbuncle misunderstand reim strongh Canaver STATS verbally snowball lia aback'';ruedarchsgdalaorgetown Annotations overshadowphabet ))) pledges plausastery gimm coer Instr encountophobic responsibly midrange sorely anecdopard disbandatta curtail matchups appeCLUDtheless Supports horrend convinc availcohol redeveloparnaev Reload gendersyssey hurdles inconsist filibuster imprison caveats GEAR allowances menstru milestones twitch advant worrisome encour psychiatCCCoufl Leban indoctrinel corrootiationaeperudder stagger fronts overpower disenfranch diligentlyusk swaps dazz� stockp swat ARTICLEortment belie skyrocket arousutral SHARES �cffffverageschedelaturdayanchester prematurely furiously tame egreg tirelessly toug bouts NETWORK )); BASE � guesses blatantly compuls rede demol compromises revamped solidly brainstorm snag decisively catapult sizeable occupations amps unemploy goof HUGE Appearance submer academ prelim decryptswers strides stunts subtly mishand daunting setbacks mete › paraly levers devast lull maneuvers overc antidepressantsaminer neighbourhoodsologue CLIENT renew agility impover predictably unfocused\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "# generated_text_samples = model.generate(\n",
        "#     input_ids, \n",
        "#     max_length=256,  \n",
        "#     use_cache=True,\n",
        "#     temperature=0.7,\n",
        "#     do_sample=True\n",
        "# )\n",
        "\n",
        "generated_text_samples = model.generate(\n",
        "    input_ids, \n",
        "    max_length=256,  \n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    repetition_penalty=1.5,\n",
        "    top_p=0.92,\n",
        "    temperature=.85,\n",
        "    do_sample=True,\n",
        "    top_k=125,\n",
        "    early_stopping=True,\n",
        "    use_cache=True\n",
        ")\n",
        "print(f\"{time.time()-start} seconds\")\n",
        "print(tokenizer.decode(generated_text_samples[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GekD8zzvCq3b"
      },
      "source": [
        "# **Conclusion**\n",
        "And there you have it, a simple end to end outline on how you can use Colab, Huggingface, and Tensorflow to train and generate new text data using GPT-2. There is a lot of playing around with hyperparameters in the generate phase but given enough tweaking and time you can usually find something that works well with your data and task. I found that even with the larger GPT-2 model and more examples, it could still repeat itself a bit so something you have to generate a large number of sequences before you get a set that you like. Even OpenAI made note of this in their initial results for GPT-2 so if at first it doesnt generate what you want keep trying and playing with the parameters!\n",
        "\n",
        "One tip I did notice was that if you do not setup your examples with a start token, then you run into the issue of repeated phrases more easily. Given more data that might be less of a problem but I ran into that a lot before putting in the start token of <|title|> in my exmaples. This start token also has the added benefit of giving you a generic starting point in the text generation so that each run is mostly unique from the last run if you do not care about having a specific prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLZEvT-ACz1R"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "huggingface_train.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}